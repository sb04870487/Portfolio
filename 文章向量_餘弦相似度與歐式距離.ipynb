{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim.models\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit# 加速套件\n",
    "import glob\n",
    "import os\n",
    "import statistics as stat\n",
    "from scipy import stats\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "import numpy as ndarray\n",
    "from gensim.models import word2vec\n",
    "#from gensim.models import Word2vec\n",
    "from gensim import models\n",
    "import logging\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "import jieba\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#載入檔案\n",
    "article = pd.read_excel(r\"C:\\Users\\Big data\\Desktop\\class\\funcardproject\\data\\文章\\新聞\\article_news_vector _final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>content</th>\n",
       "      <th>article_vector_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [No., content, article_vector_matrix]\n",
       "Index: []"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article[1:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#載入bin檔\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(datapath(r'C:\\Users\\Big data\\Desktop\\class\\funcardproject\\word2vec_model\\news\\測試100win20cbow1.bin'), binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 2.69597949e-04, -6.47094101e-04, -1.67665258e-03,\n",
       "          3.91906768e-04, -9.24433989e-05, -3.16652411e-04,\n",
       "         -1.93987612e-03,  5.08058874e-05, -2.61082314e-03,\n",
       "         -1.02852948e-03, -2.03291723e-03,  1.43862504e-03,\n",
       "          1.08168519e-03,  1.25657039e-04,  2.79157830e-04,\n",
       "          1.93173636e-03, -7.94445281e-04,  7.58122711e-04,\n",
       "         -2.01347168e-03, -4.55607769e-05,  1.99823146e-04,\n",
       "         -1.70809828e-04,  1.05148814e-04, -7.63826305e-04,\n",
       "         -1.17591070e-03,  2.14511272e-03,  1.89809466e-03,\n",
       "         -5.98302227e-04, -1.58552197e-04, -1.01821288e-03,\n",
       "         -3.17315222e-04, -3.50650633e-04, -5.69259108e-04,\n",
       "         -1.95705987e-04, -1.52082916e-03,  2.35469517e-04,\n",
       "          5.72440040e-04,  5.77591476e-04, -2.41125934e-03,\n",
       "          1.15121203e-03,  7.31202832e-04,  1.67364633e-04,\n",
       "         -2.14300992e-04,  1.07726337e-04, -6.43682026e-04,\n",
       "         -5.79928223e-04,  3.66508117e-04, -3.86958971e-04,\n",
       "         -8.15476815e-04, -8.61919543e-04, -1.74422457e-04,\n",
       "          2.78821681e-04, -4.60842493e-05, -6.39252365e-04,\n",
       "          1.52945903e-03, -7.25324382e-04, -2.71672790e-04,\n",
       "          1.50989450e-03, -1.75533656e-04, -7.71758903e-04,\n",
       "         -6.84797124e-04,  1.33368745e-03,  4.64726501e-04,\n",
       "          4.38897085e-04,  4.07475396e-04,  1.08131405e-03,\n",
       "          4.00438788e-04, -2.22719088e-03, -2.02648412e-03,\n",
       "         -3.46549903e-04, -7.88947800e-04,  6.04523637e-04,\n",
       "         -1.86722697e-04, -7.83970114e-04,  1.07135484e-03,\n",
       "         -3.30030773e-04, -3.00207408e-04,  1.48315984e-03,\n",
       "          6.57851691e-04, -6.36465207e-04, -2.38446271e-04,\n",
       "          4.56127891e-04, -1.99505128e-03, -4.71702224e-04,\n",
       "          1.65416382e-03,  7.61554344e-04, -5.65088936e-04,\n",
       "         -1.78138015e-03, -1.13696570e-03, -5.89908974e-04,\n",
       "          9.74753057e-04,  2.98977422e-04, -4.91965155e-04,\n",
       "          5.36034291e-04, -9.56385280e-04,  7.53244967e-04,\n",
       "          5.44450631e-05, -1.06167432e-03,  4.12899622e-04,\n",
       "         -2.83756497e-04]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_matrix =[get_article_matrix(article,i) for i in range(5000)]\n",
    "articles_matrix[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "菊花殘\n",
      "['菊花', '殘']\n",
      "\n",
      "這幾個字的平均向量是:\n",
      "[[ 6.13285340e-02 -2.17031449e-01 -5.13705611e-02  6.88787177e-02\n",
      "  -1.51417583e-01 -1.41463187e-02 -1.79880589e-01 -5.68504967e-02\n",
      "  -9.18727741e-03 -3.48628789e-01  7.99098015e-02  2.17369303e-01\n",
      "  -2.19995286e-02 -1.51477486e-01 -2.03162841e-02 -5.86307943e-02\n",
      "  -1.00577392e-01  1.35559082e-01 -1.31212801e-01 -3.83096784e-02\n",
      "  -9.37072486e-02  1.15839094e-01 -2.11365819e-02  1.05518058e-01\n",
      "  -2.70391405e-01  7.85563290e-02  1.19637929e-01 -1.13992460e-01\n",
      "   1.22138664e-01 -1.72064334e-01 -1.09614857e-01  1.02268413e-01\n",
      "   1.31651551e-01  2.72271819e-02 -8.51229355e-02  1.36828661e-01\n",
      "   1.84122343e-02  1.32760555e-01 -8.25339090e-03 -1.44399889e-02\n",
      "   1.86101925e-02  1.62809342e-02 -2.13433936e-01 -2.11197902e-02\n",
      "   1.90363228e-02  9.92199220e-03 -5.74173778e-03  7.42190145e-03\n",
      "  -1.27585139e-02 -1.06915116e-01 -1.18597545e-01  4.82605956e-02\n",
      "  -2.37900969e-02  1.56771958e-01  2.07881443e-02 -1.76341921e-01\n",
      "  -2.13943735e-01  7.31978863e-02 -9.98032391e-02 -1.37717873e-01\n",
      "  -2.28739724e-01  1.58803433e-01 -3.25441286e-02 -7.73436800e-02\n",
      "   2.53841132e-01  1.96154192e-01  1.21190092e-02 -3.56711686e-01\n",
      "   5.52585796e-02 -1.78694159e-01 -5.14456704e-02 -3.10626347e-02\n",
      "  -2.10267231e-01 -1.17816702e-01  2.05130383e-01 -8.74401778e-02\n",
      "   3.60096134e-02  1.91529319e-02 -6.58289790e-02  2.69055158e-01\n",
      "  -1.30694043e-02 -5.66236787e-02 -2.12447941e-01 -7.49590471e-02\n",
      "   1.49714693e-01  1.19670570e-01 -1.03196949e-02  3.13183330e-02\n",
      "  -1.53409336e-02 -5.44437990e-02  2.61004493e-02 -1.76118553e-01\n",
      "  -7.41500407e-02 -5.63339442e-02 -2.44821519e-01 -6.37478977e-02\n",
      "   4.26590145e-02 -5.48065528e-02  5.23831695e-05  1.19798496e-01]]\n",
      "第 4605 篇新聞最相似\n",
      "文章內文為: \n",
      " \n",
      " ------------------------------------------------------------ \n",
      " 「神鬼交鋒」台灣版　空中閃靈刷手栽了\n",
      "打著平行輸入，完全免稅高級舶來品，只是貨源全都在空中飛的飛機上，無本生意現在沒了，銀行將催討他之前所留下的爛賬。 回國時，他全部帶回來，再賣給下游經銷商，只要有飛到日本，香港，澳門這幾個國家的班機，都可以看見他在飛機上，也因為這樣，警方比對出入境資料和刷卡記錄，直接在登機口把他逮捕。、名牌貨通通到手，拿著刷爆的信用卡搭飛機出國去，利用飛機上的刷卡機無法和銀行連線的漏洞，刷卡購物，等到航空公司向銀行請款，銀行雖然知道這張信用卡已經被刷爆，但也只能乖乖付錢。\n"
     ]
    }
   ],
   "source": [
    "input_words= input()\n",
    "input_vector_matrix =請輸入文字(input_words)\n",
    "餘閒相似度找文章(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_article_avgvector(wordlist):\n",
    "    #取每篇文章平均向量\n",
    "    # x=np.matrix(wv_from_bin[word])安安?\n",
    "    len_wordlist = len(wordlist)\n",
    "    input_avgvector_matrix=0\n",
    "\n",
    "    for word in wordlist:\n",
    "        try:\n",
    "            x=np.matrix(wv_from_bin[word])\n",
    "            input_avgvector_matrix+=x\n",
    "            input_avgvector_matrix = input_avgvector_matrix/len_wordlist\n",
    "        except:\n",
    "            pass\n",
    "    return(input_avgvector_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 請輸入文字(input_words):\n",
    "    #隨意輸入文字\n",
    "    #轉DataFrame\n",
    "    # input_words=pd.DataFrame(input_words,columns=['content'])\n",
    "    #斷詞\n",
    "    wordlist=jieba.lcut(input_words,cut_all=False)\n",
    "    # df1=斷詞(input_words)\n",
    "    #清乾淨\n",
    "    # news = 整理(cut)\n",
    "    # 吐出來\n",
    "    # output_words = input_words['seg'][0]\n",
    "    # output_words = output_words.split(' ')\n",
    "    print(wordlist)\n",
    "    input_vector_matrix = get_article_avgvector(wordlist)\n",
    "    print()\n",
    "    print(\"這幾個字的平均向量是:\")\n",
    "    print(input_vector_matrix)\n",
    "    return(input_vector_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度\n",
    "    :param vector_a: 向量 a \n",
    "    :param vector_b: 向量 b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    vector_a = np.mat(vector_a)\n",
    "    vector_b = np.mat(vector_b)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#撈出存在excel的向量轉為matrix\n",
    "def get_article_matrix(article,i):\n",
    "    aa = article.loc[:,[\"article_vector_matrix\"]][i:i+1]\n",
    "    #轉ARRAY再轉list\n",
    "    b = np.array(aa)\n",
    "    b=b[0].tolist()#list\n",
    "    # 切\n",
    "    c = str(b[0]).split(',')\n",
    "    article_matrix = np.mat(c).astype(float)\n",
    "    return(article_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 餘閒相似度找文章(input_words):\n",
    "    articles_matrix_list=[]\n",
    "    for b in range(5000):\n",
    "        結果=cos_sim(input_vector_matrix,articles_matrix[b])\n",
    "        articles_matrix_list.append(結果)\n",
    "    print(\"第\",articles_matrix_list.index(max(articles_matrix_list)),\"篇新聞最相似\")\n",
    "    哭喔 = articles_matrix_list.index(max(articles_matrix_list))\n",
    "    print(\"文章內文為:\",\"\\n\",\"\\n\",\"------------------------------------------------------------\",\"\\n\",np.array(article[哭喔:哭喔+1]['content'])[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 歐式距離找文章(input_words):\n",
    "    articles_matrix_list=[]\n",
    "    for b in range(5000):\n",
    "        結果=np.linalg.norm(input_vector_matrix - articles_matrix[b])\n",
    "        articles_matrix_list.append(結果)\n",
    "    print(\"第\",articles_matrix_list.index(max(articles_matrix_list)),\"篇新聞最相似\")\n",
    "    print(\"文章內文為:\",\"\\n\",\"\\n\",\"------------------------------------------------------------\",\"\\n\",np.array(article[:1]['content'])[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斷詞套件\n",
    "def 斷詞(news):\n",
    "    df1=news\n",
    "    df1['seg']=None\n",
    "    df1['word_freq']=None\n",
    "    n=len(news)\n",
    "    alltext=[]\n",
    "    stopset=set()\n",
    "    stop2=['/n','']\n",
    "    with open('C:\\\\Users\\\\Big data\\\\Desktop\\\\字詞貼標\\\\DB104-master\\\\stop.txt','r',encoding='ISO-8859-1') as s:\n",
    "        for line in s:\n",
    "            stopset.add(line.strip('\\n'))\n",
    "    jieba.load_userdict('C:\\\\Users\\\\Big data\\\\Desktop\\\\字詞貼標\\\\DB104-master\\\\userdict.txt')\n",
    "    for i in range(n):\n",
    "        seg=''\n",
    "        wf={}\n",
    "        text= str(df1.loc[i]['content'])\n",
    "        cut=jieba.cut(text,cut_all=False)\n",
    "        for j in cut:\n",
    "            if j not in stopset:\n",
    "                seg +=j+' '\n",
    "                seg = seg.replace(\"\\n\",'')\n",
    "        df1['seg'][i]=seg\n",
    "        for w in seg.split(' '):\n",
    "            if w not in wf:\n",
    "                wf[w]=1\n",
    "            else:\n",
    "                wf[w]+=1\n",
    "        word_list=[(k,wf[k]) for k in wf if k not in stop2 ]\n",
    "        word_list.sort(key=lambda a :a[1],reverse=True)\n",
    "        df1['word_freq'][i]=word_list\n",
    "    return(df1)\n",
    "\n",
    "\n",
    "\n",
    "# 斷詞整理(去除標點符號)\n",
    "def 整理(df1):\n",
    "    datanews = df1[\"seg\"]\n",
    "    # 轉list\n",
    "    train_data = np.array(datanews)#np.ndarray()\n",
    "    datanews=train_data.tolist()#list\n",
    "    df1=''\n",
    "    for datanew in datanews:\n",
    "        df1+=str(datanew)+'\\n'\n",
    "    df1=df1.replace('[','').replace('\\'','').replace('「 ','').replace('」 ','').replace('」 ','').replace('、 ','').replace('： ','')\n",
    "    df1=df1.replace('《 ','').replace('》 ','').replace('） ','').replace('( ','').replace('／ ','').replace('， ','')\n",
    "    df1=df1.replace('╱ ','').replace('！ ','').replace('？ ','').replace('（ ','').replace('。 ','').replace('； ','').replace('… ','')\n",
    "    df1=df1.replace(']','').replace(' :','').replace('\\u3000','').replace('\\n','')\n",
    "    df2=df1\n",
    "    for i in range(len(df2)):\n",
    "        df1=' '.join(df2.split())\n",
    "    return(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
