{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mse會嚴重受到極端值影響,因此去除和卡額度100萬以上的資料,約8筆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#複製 https://github.com/agu3rra/NeuralNetwork-RegressionExample/blob/master/Tutorial.ipynb\n",
    "from keras.datasets import boston_housing\n",
    "from keras.datasets import reuters\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import regularizers\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>serveTime</th>\n",
       "      <th>credLimit</th>\n",
       "      <th>Loan</th>\n",
       "      <th>SalPerY</th>\n",
       "      <th>holdCard</th>\n",
       "      <th>Career</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.137854</td>\n",
       "      <td>42.395304</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.645888e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.713098e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000e+05</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>38.687619</td>\n",
       "      <td>500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.818209e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34.333205</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.700000e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000e+05</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>11.490740</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.263163e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.966329e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.559199e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.670738</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.639850e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.150100e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.160804e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.712941e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          age   serveTime  credLimit      Loan       SalPerY  holdCard  Career\n",
       "0   33.137854   42.395304     500000  0.000000  1.645888e+06         0       2\n",
       "1   40.000000  120.000000     500000  2.000000  2.500000e+06         0       4\n",
       "2   35.000000  108.000000     500000  0.000000  5.000000e+06         0       2\n",
       "3   36.000000   84.000000     500000  0.000000  1.713098e+06         1       2\n",
       "4   29.000000   30.000000     500000  2.000000  5.000000e+05         1       2\n",
       "5   30.000000   38.687619     500000  2.000000  1.818209e+06         1       2\n",
       "6   34.333205   48.000000     500000  2.000000  1.700000e+06         1       2\n",
       "7   30.000000   30.000000     500000  2.000000  5.000000e+05         1       2\n",
       "8   23.000000   11.490740     500000  0.000000  1.263163e+06         1       2\n",
       "9   54.000000   10.000000     500000  2.000000  1.966329e+06         1       2\n",
       "10  35.000000   72.000000     500000  0.000000  1.559199e+06         1       2\n",
       "11  33.000000   48.000000     500000  0.670738  6.000000e+05         1       2\n",
       "12  34.000000  124.000000     500000  3.000000  4.000000e+06         1       2\n",
       "13  32.000000   84.000000     500000  0.000000  1.639850e+06         1       4\n",
       "14  30.000000    6.000000     500000  0.000000  6.000000e+05         1       2\n",
       "15  27.000000   12.000000     500000  0.000000  1.150100e+06         1       2\n",
       "16  39.000000   54.000000     500000  3.000000  1.000000e+06         1       4\n",
       "17  25.000000   24.000000     500000  0.000000  1.160804e+06         1       2\n",
       "18  35.000000  108.000000     500000  0.000000  5.000000e+06         1       2\n",
       "19  30.000000   36.000000     500000  2.000000  1.712941e+06         1       4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\Big data\\Desktop\\class\\funcardproject\\補值data\\補值_rf_Y為數值_去除100萬以上.xls',encoding='utf-16')\n",
    "df = df.loc[:, [\"age\",\"serveTime\",\"credLimit\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\"]] \n",
    "df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1417, 6)\n",
      "(608, 6)\n",
      "(1417, 1)\n",
      "(608, 1)\n"
     ]
    }
   ],
   "source": [
    "#先打散資料(三次)\n",
    "for i in range(3):\n",
    "    df = shuffle(df)\n",
    "#再切成訓練與測試\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(df.loc[:, [\"age\",\"serveTime\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\"]] , df.loc[:, [\"credLimit\"]] , test_size=0.3, random_state=42)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_targets.shape)\n",
    "print(test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_targets, test_targets = train_test_split(df.loc[:, [\"age\",\"serveTime\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\"]] , df.loc[:, [\"credLimit\"]] , test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正規化\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -=mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data/=std\n",
    "test_data-=mean\n",
    "test_data/=std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(8, activation='relu',input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(8, activation='relu'))\n",
    "    model.add(layers.Dense(1))#非0~1的預測,故不使用啟動函數轉為01\n",
    "    model.compile(optimizer='rmsprop',loss='mae',metrics=['mae'])\n",
    "    return model\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=4\n",
    "num_val_samples = len(train_data)//k\n",
    "num_epochs = 200\n",
    "all_scores=[]\n",
    "num_val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 1063 samples, validate on 354 samples\n",
      "Epoch 1/200\n",
      "1063/1063 [==============================] - 0s 313us/step - loss: 135251.5627 - mae: 135251.5312 - val_loss: 136906.9431 - val_mae: 136906.9375\n",
      "Epoch 2/200\n",
      "1063/1063 [==============================] - 0s 109us/step - loss: 135248.1961 - mae: 135248.1719 - val_loss: 136902.5448 - val_mae: 136902.5312\n",
      "Epoch 3/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 135241.9736 - mae: 135241.9844 - val_loss: 136894.9953 - val_mae: 136895.0156\n",
      "Epoch 4/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 135231.9424 - mae: 135231.9219 - val_loss: 136883.2721 - val_mae: 136883.2812\n",
      "Epoch 5/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 135216.7923 - mae: 135216.7969 - val_loss: 136865.8746 - val_mae: 136865.8906\n",
      "Epoch 6/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 135194.8725 - mae: 135194.8750 - val_loss: 136841.8825 - val_mae: 136841.8906\n",
      "Epoch 7/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 135165.7036 - mae: 135165.7344 - val_loss: 136810.4597 - val_mae: 136810.4688\n",
      "Epoch 8/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 135128.1206 - mae: 135128.0938 - val_loss: 136770.6018 - val_mae: 136770.5938\n",
      "Epoch 9/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 135080.8559 - mae: 135080.8438 - val_loss: 136720.8886 - val_mae: 136720.8906\n",
      "Epoch 10/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 135023.2525 - mae: 135023.2812 - val_loss: 136660.7801 - val_mae: 136660.7812\n",
      "Epoch 11/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 134953.9369 - mae: 134953.9531 - val_loss: 136588.6568 - val_mae: 136588.6719\n",
      "Epoch 12/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 134870.9241 - mae: 134870.9062 - val_loss: 136503.8053 - val_mae: 136503.7812\n",
      "Epoch 13/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 134774.1547 - mae: 134774.1719 - val_loss: 136404.8861 - val_mae: 136404.8906\n",
      "Epoch 14/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 134662.3294 - mae: 134662.3438 - val_loss: 136290.8861 - val_mae: 136290.9219\n",
      "Epoch 15/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 134534.9070 - mae: 134534.8750 - val_loss: 136161.3549 - val_mae: 136161.3594\n",
      "Epoch 16/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 134388.6417 - mae: 134388.6250 - val_loss: 136013.7482 - val_mae: 136013.7188\n",
      "Epoch 17/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 134224.1339 - mae: 134224.0938 - val_loss: 135848.6881 - val_mae: 135848.7031\n",
      "Epoch 18/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 134040.7599 - mae: 134040.7656 - val_loss: 135663.5280 - val_mae: 135663.5000\n",
      "Epoch 19/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 133834.8878 - mae: 133834.9062 - val_loss: 135458.3526 - val_mae: 135458.3438\n",
      "Epoch 20/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 133608.4841 - mae: 133608.4844 - val_loss: 135231.6395 - val_mae: 135231.6094\n",
      "Epoch 21/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 133358.3508 - mae: 133358.3750 - val_loss: 134983.0108 - val_mae: 134983.0312\n",
      "Epoch 22/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 133088.5703 - mae: 133088.5469 - val_loss: 134713.5559 - val_mae: 134713.5469\n",
      "Epoch 23/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 132791.0099 - mae: 132791.0156 - val_loss: 134415.8566 - val_mae: 134415.8438\n",
      "Epoch 24/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 132465.9054 - mae: 132465.9219 - val_loss: 134092.2023 - val_mae: 134092.1875\n",
      "Epoch 25/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 132114.2288 - mae: 132114.2188 - val_loss: 133742.5045 - val_mae: 133742.5156\n",
      "Epoch 26/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 131732.4356 - mae: 131732.4062 - val_loss: 133365.0053 - val_mae: 133365.0000\n",
      "Epoch 27/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 131321.0050 - mae: 131321.0000 - val_loss: 132957.5363 - val_mae: 132957.5312\n",
      "Epoch 28/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 130882.9299 - mae: 130882.9766 - val_loss: 132522.0921 - val_mae: 132522.1094\n",
      "Epoch 29/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 130409.1475 - mae: 130409.1641 - val_loss: 132056.1329 - val_mae: 132056.1250\n",
      "Epoch 30/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 129903.0501 - mae: 129903.0391 - val_loss: 131554.5186 - val_mae: 131554.5000\n",
      "Epoch 31/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 129358.8050 - mae: 129358.7969 - val_loss: 131019.6553 - val_mae: 131019.6406\n",
      "Epoch 32/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 128784.8083 - mae: 128784.7969 - val_loss: 130449.8171 - val_mae: 130449.7969\n",
      "Epoch 33/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 128172.2073 - mae: 128172.2031 - val_loss: 129849.1127 - val_mae: 129849.1172\n",
      "Epoch 34/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 127520.7611 - mae: 127520.7656 - val_loss: 129209.1212 - val_mae: 129209.1328\n",
      "Epoch 35/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 126831.5537 - mae: 126831.5312 - val_loss: 128528.4807 - val_mae: 128528.4844\n",
      "Epoch 36/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 126103.2107 - mae: 126103.2422 - val_loss: 127814.8389 - val_mae: 127814.8359\n",
      "Epoch 37/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 125337.1937 - mae: 125337.2031 - val_loss: 127056.3588 - val_mae: 127056.3750\n",
      "Epoch 38/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 124526.9663 - mae: 124526.9453 - val_loss: 126262.9847 - val_mae: 126262.9844\n",
      "Epoch 39/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 123681.9400 - mae: 123681.9453 - val_loss: 125454.7393 - val_mae: 125454.7109\n",
      "Epoch 40/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 122854.2877 - mae: 122854.3359 - val_loss: 124696.1181 - val_mae: 124696.1094\n",
      "Epoch 41/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 122075.1222 - mae: 122075.1406 - val_loss: 123946.8583 - val_mae: 123946.8672\n",
      "Epoch 42/200\n",
      "1063/1063 [==============================] - 0s 129us/step - loss: 121307.5592 - mae: 121307.5625 - val_loss: 123204.1172 - val_mae: 123204.1250\n",
      "Epoch 43/200\n",
      "1063/1063 [==============================] - 0s 107us/step - loss: 120521.7425 - mae: 120521.7422 - val_loss: 122424.2839 - val_mae: 122424.2969\n",
      "Epoch 44/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 119727.2238 - mae: 119727.2266 - val_loss: 121625.4344 - val_mae: 121625.4219\n",
      "Epoch 45/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 118913.2570 - mae: 118913.2422 - val_loss: 120791.5904 - val_mae: 120791.5938\n",
      "Epoch 46/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 118072.7010 - mae: 118072.7344 - val_loss: 119965.7787 - val_mae: 119965.7734\n",
      "Epoch 47/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 117240.0866 - mae: 117240.0938 - val_loss: 119134.8644 - val_mae: 119134.8594\n",
      "Epoch 48/200\n",
      "1063/1063 [==============================] - 0s 129us/step - loss: 116372.0690 - mae: 116372.0938 - val_loss: 118280.0636 - val_mae: 118280.0312\n",
      "Epoch 49/200\n",
      "1063/1063 [==============================] - 0s 115us/step - loss: 115469.4729 - mae: 115469.4766 - val_loss: 117407.9031 - val_mae: 117407.9062\n",
      "Epoch 50/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 114559.3257 - mae: 114559.3203 - val_loss: 116507.3679 - val_mae: 116507.3594\n",
      "Epoch 51/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 113631.8235 - mae: 113631.8359 - val_loss: 115583.3897 - val_mae: 115583.3906\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 114us/step - loss: 112656.0275 - mae: 112656.0234 - val_loss: 114611.4653 - val_mae: 114611.4688\n",
      "Epoch 53/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 111646.3401 - mae: 111646.3203 - val_loss: 113629.4580 - val_mae: 113629.4688\n",
      "Epoch 54/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 110604.5657 - mae: 110604.5469 - val_loss: 112603.1398 - val_mae: 112603.1328\n",
      "Epoch 55/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 109516.1014 - mae: 109516.1094 - val_loss: 111565.8064 - val_mae: 111565.7969\n",
      "Epoch 56/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 108421.2605 - mae: 108421.2656 - val_loss: 110533.1003 - val_mae: 110533.1094\n",
      "Epoch 57/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 107312.4721 - mae: 107312.4844 - val_loss: 109492.0660 - val_mae: 109492.0781\n",
      "Epoch 58/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 106185.4004 - mae: 106185.4219 - val_loss: 108374.0510 - val_mae: 108374.0547\n",
      "Epoch 59/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 105047.0884 - mae: 105047.0938 - val_loss: 107281.5109 - val_mae: 107281.4922\n",
      "Epoch 60/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 103868.1150 - mae: 103868.1016 - val_loss: 106131.6201 - val_mae: 106131.6250\n",
      "Epoch 61/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 102638.8551 - mae: 102638.8672 - val_loss: 104936.5892 - val_mae: 104936.6016\n",
      "Epoch 62/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 101399.6697 - mae: 101399.6875 - val_loss: 103750.5834 - val_mae: 103750.5625\n",
      "Epoch 63/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 100139.7873 - mae: 100139.7969 - val_loss: 102512.1329 - val_mae: 102512.1250\n",
      "Epoch 64/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 98831.8133 - mae: 98831.8359 - val_loss: 101238.9560 - val_mae: 101238.9609\n",
      "Epoch 65/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 97498.2322 - mae: 97498.2109 - val_loss: 99932.1749 - val_mae: 99932.1797\n",
      "Epoch 66/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 96144.2152 - mae: 96144.2188 - val_loss: 98634.2459 - val_mae: 98634.2344\n",
      "Epoch 67/200\n",
      "1063/1063 [==============================] - 0s 110us/step - loss: 94807.0171 - mae: 94807.0469 - val_loss: 97359.6481 - val_mae: 97359.6484\n",
      "Epoch 68/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 93480.1882 - mae: 93480.1797 - val_loss: 96031.1674 - val_mae: 96031.1641\n",
      "Epoch 69/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 92144.5595 - mae: 92144.5078 - val_loss: 94754.1012 - val_mae: 94754.1016\n",
      "Epoch 70/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 90763.4970 - mae: 90763.5078 - val_loss: 93482.7959 - val_mae: 93482.7891\n",
      "Epoch 71/200\n",
      "1063/1063 [==============================] - 0s 108us/step - loss: 89373.2964 - mae: 89373.2969 - val_loss: 92249.4203 - val_mae: 92249.4297\n",
      "Epoch 72/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 87992.1831 - mae: 87992.1875 - val_loss: 91032.9514 - val_mae: 91032.9609\n",
      "Epoch 73/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 86652.0401 - mae: 86652.0312 - val_loss: 89843.1838 - val_mae: 89843.1953\n",
      "Epoch 74/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 85304.2678 - mae: 85304.2734 - val_loss: 88682.3776 - val_mae: 88682.3750\n",
      "Epoch 75/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 83987.2774 - mae: 83987.2734 - val_loss: 87625.2399 - val_mae: 87625.2344\n",
      "Epoch 76/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 82722.6957 - mae: 82722.7109 - val_loss: 86596.7637 - val_mae: 86596.7578\n",
      "Epoch 77/200\n",
      "1063/1063 [==============================] - 0s 108us/step - loss: 81503.8413 - mae: 81503.8438 - val_loss: 85606.9451 - val_mae: 85606.9453\n",
      "Epoch 78/200\n",
      "1063/1063 [==============================] - 0s 108us/step - loss: 80297.9626 - mae: 80297.9531 - val_loss: 84637.3619 - val_mae: 84637.3594\n",
      "Epoch 79/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 79196.8966 - mae: 79196.9062 - val_loss: 83718.2869 - val_mae: 83718.2812\n",
      "Epoch 80/200\n",
      "1063/1063 [==============================] - 0s 111us/step - loss: 78155.0428 - mae: 78155.0234 - val_loss: 82887.7756 - val_mae: 82887.7734\n",
      "Epoch 81/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 77168.6099 - mae: 77168.6172 - val_loss: 82056.8570 - val_mae: 82056.8594\n",
      "Epoch 82/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 76204.2488 - mae: 76204.2422 - val_loss: 81235.2027 - val_mae: 81235.2109\n",
      "Epoch 83/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 75286.5287 - mae: 75286.5469 - val_loss: 80466.7980 - val_mae: 80466.7891\n",
      "Epoch 84/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 74385.9101 - mae: 74385.9219 - val_loss: 79726.7021 - val_mae: 79726.7031\n",
      "Epoch 85/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 73507.6465 - mae: 73507.6406 - val_loss: 79036.8483 - val_mae: 79036.8594\n",
      "Epoch 86/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 72724.9592 - mae: 72724.9609 - val_loss: 78357.8639 - val_mae: 78357.8594\n",
      "Epoch 87/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 72021.4888 - mae: 72021.4688 - val_loss: 77787.5893 - val_mae: 77787.5703\n",
      "Epoch 88/200\n",
      "1063/1063 [==============================] - 0s 96us/step - loss: 71370.7813 - mae: 71370.7734 - val_loss: 77232.7605 - val_mae: 77232.7422\n",
      "Epoch 89/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 70806.5334 - mae: 70806.5469 - val_loss: 76737.8064 - val_mae: 76737.8047\n",
      "Epoch 90/200\n",
      "1063/1063 [==============================] - 0s 108us/step - loss: 70266.7278 - mae: 70266.7500 - val_loss: 76262.0847 - val_mae: 76262.1016\n",
      "Epoch 91/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 69776.8004 - mae: 69776.7969 - val_loss: 75817.2076 - val_mae: 75817.2188\n",
      "Epoch 92/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 69310.7755 - mae: 69310.7656 - val_loss: 75376.9493 - val_mae: 75376.9609\n",
      "Epoch 93/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 68853.8338 - mae: 68853.8359 - val_loss: 74951.7670 - val_mae: 74951.7734\n",
      "Epoch 94/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 68393.3718 - mae: 68393.3906 - val_loss: 74541.0686 - val_mae: 74541.0703\n",
      "Epoch 95/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 67976.2721 - mae: 67976.2422 - val_loss: 74129.0868 - val_mae: 74129.0781\n",
      "Epoch 96/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 67563.6168 - mae: 67563.6016 - val_loss: 73730.2629 - val_mae: 73730.2578\n",
      "Epoch 97/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 67158.1585 - mae: 67158.1484 - val_loss: 73329.2852 - val_mae: 73329.2812\n",
      "Epoch 98/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 66772.8221 - mae: 66772.8203 - val_loss: 72972.2202 - val_mae: 72972.2188\n",
      "Epoch 99/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 66427.4025 - mae: 66427.3984 - val_loss: 72627.6120 - val_mae: 72627.6172\n",
      "Epoch 100/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 66100.9694 - mae: 66100.9609 - val_loss: 72299.8402 - val_mae: 72299.8281\n",
      "Epoch 101/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 65787.4279 - mae: 65787.4297 - val_loss: 71998.1539 - val_mae: 71998.1641\n",
      "Epoch 102/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 65512.3377 - mae: 65512.3242 - val_loss: 71733.9249 - val_mae: 71733.9297\n",
      "Epoch 103/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 65249.1323 - mae: 65249.1211 - val_loss: 71467.9641 - val_mae: 71467.9453\n",
      "Epoch 104/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 65008.1510 - mae: 65008.1445 - val_loss: 71227.1425 - val_mae: 71227.1562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 64785.3825 - mae: 64785.3828 - val_loss: 70993.0883 - val_mae: 70993.0859\n",
      "Epoch 106/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 64570.9495 - mae: 64570.9414 - val_loss: 70766.2648 - val_mae: 70766.2656\n",
      "Epoch 107/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 64377.1570 - mae: 64377.1680 - val_loss: 70543.3158 - val_mae: 70543.3125\n",
      "Epoch 108/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 64191.6986 - mae: 64191.6992 - val_loss: 70333.8564 - val_mae: 70333.8594\n",
      "Epoch 109/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 64025.5193 - mae: 64025.5195 - val_loss: 70135.4195 - val_mae: 70135.4141\n",
      "Epoch 110/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 63866.1968 - mae: 63866.1992 - val_loss: 69944.2000 - val_mae: 69944.1953\n",
      "Epoch 111/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 63714.1124 - mae: 63714.1133 - val_loss: 69775.0672 - val_mae: 69775.0781\n",
      "Epoch 112/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 63572.3429 - mae: 63572.3516 - val_loss: 69640.1799 - val_mae: 69640.1953\n",
      "Epoch 113/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 63444.5122 - mae: 63444.5078 - val_loss: 69506.4193 - val_mae: 69506.4219\n",
      "Epoch 114/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 63310.0627 - mae: 63310.0586 - val_loss: 69371.9272 - val_mae: 69371.9219\n",
      "Epoch 115/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 63180.2013 - mae: 63180.2070 - val_loss: 69244.5627 - val_mae: 69244.5625\n",
      "Epoch 116/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 63058.9127 - mae: 63058.9141 - val_loss: 69109.8270 - val_mae: 69109.8281\n",
      "Epoch 117/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 62942.1636 - mae: 62942.1562 - val_loss: 68984.7867 - val_mae: 68984.7812\n",
      "Epoch 118/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 62815.4223 - mae: 62815.4023 - val_loss: 68851.7204 - val_mae: 68851.7188\n",
      "Epoch 119/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 62701.7193 - mae: 62701.7109 - val_loss: 68733.2887 - val_mae: 68733.2969\n",
      "Epoch 120/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 62585.0857 - mae: 62585.0938 - val_loss: 68604.5533 - val_mae: 68604.5625\n",
      "Epoch 121/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 62475.6024 - mae: 62475.5977 - val_loss: 68489.0102 - val_mae: 68489.0078\n",
      "Epoch 122/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 62361.3831 - mae: 62361.3867 - val_loss: 68367.9880 - val_mae: 68367.9766\n",
      "Epoch 123/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 62252.4129 - mae: 62252.4180 - val_loss: 68249.8012 - val_mae: 68249.8047\n",
      "Epoch 124/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 62146.8704 - mae: 62146.8750 - val_loss: 68142.3919 - val_mae: 68142.3984\n",
      "Epoch 125/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 62039.7446 - mae: 62039.7344 - val_loss: 68035.5273 - val_mae: 68035.5234\n",
      "Epoch 126/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 61933.6234 - mae: 61933.6328 - val_loss: 67930.4484 - val_mae: 67930.4453\n",
      "Epoch 127/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 61832.4683 - mae: 61832.4648 - val_loss: 67833.2429 - val_mae: 67833.2500\n",
      "Epoch 128/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 61732.4475 - mae: 61732.4414 - val_loss: 67732.2959 - val_mae: 67732.2969\n",
      "Epoch 129/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 61630.4773 - mae: 61630.4883 - val_loss: 67627.1091 - val_mae: 67627.1250\n",
      "Epoch 130/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 61527.4845 - mae: 61527.4922 - val_loss: 67525.6132 - val_mae: 67525.6172\n",
      "Epoch 131/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 61429.4915 - mae: 61429.4844 - val_loss: 67427.7796 - val_mae: 67427.7969\n",
      "Epoch 132/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 61326.2809 - mae: 61326.2812 - val_loss: 67328.5429 - val_mae: 67328.5391\n",
      "Epoch 133/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 61221.2396 - mae: 61221.2500 - val_loss: 67237.8027 - val_mae: 67237.8047\n",
      "Epoch 134/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 61126.1089 - mae: 61126.1016 - val_loss: 67156.9171 - val_mae: 67156.9062\n",
      "Epoch 135/200\n",
      "1063/1063 [==============================] - 0s 111us/step - loss: 61027.9295 - mae: 61027.9414 - val_loss: 67071.9865 - val_mae: 67071.9844\n",
      "Epoch 136/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 60941.7026 - mae: 60941.6914 - val_loss: 66996.5041 - val_mae: 66996.5000\n",
      "Epoch 137/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 60882.3022 - mae: 60882.3047 - val_loss: 66940.4214 - val_mae: 66940.4219\n",
      "Epoch 138/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 60834.7151 - mae: 60834.7070 - val_loss: 66885.2329 - val_mae: 66885.2188\n",
      "Epoch 139/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60783.7436 - mae: 60783.7422 - val_loss: 66838.5120 - val_mae: 66838.5000\n",
      "Epoch 140/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 60733.6544 - mae: 60733.6445 - val_loss: 66785.9683 - val_mae: 66785.9688\n",
      "Epoch 141/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 60682.6913 - mae: 60682.6758 - val_loss: 66743.1174 - val_mae: 66743.1172\n",
      "Epoch 142/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 60637.4340 - mae: 60637.4414 - val_loss: 66702.1488 - val_mae: 66702.1484\n",
      "Epoch 143/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 60594.6152 - mae: 60594.6289 - val_loss: 66670.0088 - val_mae: 66670.0156\n",
      "Epoch 144/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60554.6756 - mae: 60554.6719 - val_loss: 66634.7212 - val_mae: 66634.7266\n",
      "Epoch 145/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 60515.8290 - mae: 60515.8047 - val_loss: 66600.7875 - val_mae: 66600.7891\n",
      "Epoch 146/200\n",
      "1063/1063 [==============================] - 0s 111us/step - loss: 60475.6563 - mae: 60475.6641 - val_loss: 66568.1950 - val_mae: 66568.1875\n",
      "Epoch 147/200\n",
      "1063/1063 [==============================] - 0s 107us/step - loss: 60439.7823 - mae: 60439.7656 - val_loss: 66536.2748 - val_mae: 66536.2734\n",
      "Epoch 148/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 60399.3715 - mae: 60399.3594 - val_loss: 66502.8933 - val_mae: 66502.8906\n",
      "Epoch 149/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 60360.4337 - mae: 60360.4414 - val_loss: 66470.5845 - val_mae: 66470.5781\n",
      "Epoch 150/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60324.4677 - mae: 60324.4688 - val_loss: 66431.1635 - val_mae: 66431.1562\n",
      "Epoch 151/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60283.5129 - mae: 60283.4883 - val_loss: 66387.6060 - val_mae: 66387.5938\n",
      "Epoch 152/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60252.5375 - mae: 60252.5312 - val_loss: 66354.0326 - val_mae: 66354.0391\n",
      "Epoch 153/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 60217.4662 - mae: 60217.4727 - val_loss: 66324.9190 - val_mae: 66324.9297\n",
      "Epoch 154/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60186.4123 - mae: 60186.4023 - val_loss: 66288.7473 - val_mae: 66288.7422\n",
      "Epoch 155/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60152.6965 - mae: 60152.6953 - val_loss: 66256.1037 - val_mae: 66256.1094\n",
      "Epoch 156/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 60118.7371 - mae: 60118.7422 - val_loss: 66224.0594 - val_mae: 66224.0703\n",
      "Epoch 157/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 60085.1616 - mae: 60085.1680 - val_loss: 66194.1100 - val_mae: 66194.1094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 60051.0155 - mae: 60051.0156 - val_loss: 66166.1534 - val_mae: 66166.1562\n",
      "Epoch 159/200\n",
      "1063/1063 [==============================] - 0s 116us/step - loss: 60016.9269 - mae: 60016.9219 - val_loss: 66139.5279 - val_mae: 66139.5391\n",
      "Epoch 160/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 59982.3245 - mae: 59982.3359 - val_loss: 66109.4122 - val_mae: 66109.4219\n",
      "Epoch 161/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59950.3085 - mae: 59950.3047 - val_loss: 66079.8908 - val_mae: 66079.8906\n",
      "Epoch 162/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 59915.5731 - mae: 59915.5859 - val_loss: 66042.7384 - val_mae: 66042.7422\n",
      "Epoch 163/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 59882.9659 - mae: 59882.9688 - val_loss: 66012.2678 - val_mae: 66012.2578\n",
      "Epoch 164/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 59851.0160 - mae: 59851.0039 - val_loss: 65982.6513 - val_mae: 65982.6562\n",
      "Epoch 165/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 59819.9162 - mae: 59819.9141 - val_loss: 65952.0341 - val_mae: 65952.0469\n",
      "Epoch 166/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 59791.3108 - mae: 59791.3125 - val_loss: 65928.2228 - val_mae: 65928.2266\n",
      "Epoch 167/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 59763.3976 - mae: 59763.3945 - val_loss: 65901.6924 - val_mae: 65901.6797\n",
      "Epoch 168/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 59735.0828 - mae: 59735.0625 - val_loss: 65867.4876 - val_mae: 65867.4766\n",
      "Epoch 169/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 59709.7598 - mae: 59709.7695 - val_loss: 65832.8365 - val_mae: 65832.8438\n",
      "Epoch 170/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 59682.0521 - mae: 59682.0508 - val_loss: 65807.2689 - val_mae: 65807.2578\n",
      "Epoch 171/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 59656.4429 - mae: 59656.4492 - val_loss: 65780.6300 - val_mae: 65780.6328\n",
      "Epoch 172/200\n",
      "1063/1063 [==============================] - 0s 111us/step - loss: 59628.0881 - mae: 59628.0859 - val_loss: 65756.2631 - val_mae: 65756.2656\n",
      "Epoch 173/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59599.5426 - mae: 59599.5430 - val_loss: 65731.8098 - val_mae: 65731.8203\n",
      "Epoch 174/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59572.6972 - mae: 59572.7031 - val_loss: 65708.2317 - val_mae: 65708.2344\n",
      "Epoch 175/200\n",
      "1063/1063 [==============================] - 0s 108us/step - loss: 59542.6817 - mae: 59542.6680 - val_loss: 65682.0581 - val_mae: 65682.0547\n",
      "Epoch 176/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 59516.9096 - mae: 59516.8945 - val_loss: 65654.9463 - val_mae: 65654.9453\n",
      "Epoch 177/200\n",
      "1063/1063 [==============================] - 0s 109us/step - loss: 59493.5628 - mae: 59493.5625 - val_loss: 65635.4507 - val_mae: 65635.4375\n",
      "Epoch 178/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59467.1522 - mae: 59467.1367 - val_loss: 65611.6351 - val_mae: 65611.6406\n",
      "Epoch 179/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 59444.4095 - mae: 59444.4062 - val_loss: 65594.6013 - val_mae: 65594.6016\n",
      "Epoch 180/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 59427.3522 - mae: 59427.3477 - val_loss: 65566.9608 - val_mae: 65566.9609\n",
      "Epoch 181/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 59406.0095 - mae: 59406.0117 - val_loss: 65547.2372 - val_mae: 65547.2344\n",
      "Epoch 182/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59385.4560 - mae: 59385.4688 - val_loss: 65525.3899 - val_mae: 65525.3828\n",
      "Epoch 183/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59366.5186 - mae: 59366.5117 - val_loss: 65504.5676 - val_mae: 65504.5703\n",
      "Epoch 184/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 59346.4311 - mae: 59346.4219 - val_loss: 65485.6551 - val_mae: 65485.6562\n",
      "Epoch 185/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59326.5794 - mae: 59326.5859 - val_loss: 65467.4709 - val_mae: 65467.4727\n",
      "Epoch 186/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59309.5951 - mae: 59309.5898 - val_loss: 65442.7764 - val_mae: 65442.7812\n",
      "Epoch 187/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 59291.8131 - mae: 59291.8203 - val_loss: 65417.0082 - val_mae: 65417.0000\n",
      "Epoch 188/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 59274.9339 - mae: 59274.9297 - val_loss: 65399.8827 - val_mae: 65399.8828\n",
      "Epoch 189/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 59259.9222 - mae: 59259.9180 - val_loss: 65388.3878 - val_mae: 65388.3906\n",
      "Epoch 190/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59243.0932 - mae: 59243.0977 - val_loss: 65370.6291 - val_mae: 65370.6328\n",
      "Epoch 191/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 59228.8420 - mae: 59228.8516 - val_loss: 65352.1570 - val_mae: 65352.1680\n",
      "Epoch 192/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 59211.6934 - mae: 59211.6992 - val_loss: 65334.3434 - val_mae: 65334.3398\n",
      "Epoch 193/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59198.5871 - mae: 59198.5781 - val_loss: 65320.4509 - val_mae: 65320.4531\n",
      "Epoch 194/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 59181.1460 - mae: 59181.1445 - val_loss: 65305.6440 - val_mae: 65305.6484\n",
      "Epoch 195/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 59166.1506 - mae: 59166.1484 - val_loss: 65293.0116 - val_mae: 65293.0117\n",
      "Epoch 196/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59152.6400 - mae: 59152.6484 - val_loss: 65270.7519 - val_mae: 65270.7500\n",
      "Epoch 197/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59138.5966 - mae: 59138.6016 - val_loss: 65252.2749 - val_mae: 65252.2617\n",
      "Epoch 198/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 59123.2176 - mae: 59123.2266 - val_loss: 65239.1414 - val_mae: 65239.1406\n",
      "Epoch 199/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 59109.5370 - mae: 59109.5508 - val_loss: 65226.1498 - val_mae: 65226.1562\n",
      "Epoch 200/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 59096.2121 - mae: 59096.2266 - val_loss: 65210.7159 - val_mae: 65210.7109\n",
      "processing fold # 1\n",
      "Train on 1063 samples, validate on 354 samples\n",
      "Epoch 1/200\n",
      "1063/1063 [==============================] - 0s 162us/step - loss: 135717.5274 - mae: 135717.5469 - val_loss: 135509.0606 - val_mae: 135509.0781\n",
      "Epoch 2/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 135715.1056 - mae: 135715.1094 - val_loss: 135505.8168 - val_mae: 135505.8438\n",
      "Epoch 3/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 135711.0006 - mae: 135711.0156 - val_loss: 135500.5492 - val_mae: 135500.5781\n",
      "Epoch 4/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 135704.7034 - mae: 135704.7031 - val_loss: 135492.5529 - val_mae: 135492.5469\n",
      "Epoch 5/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 135695.1992 - mae: 135695.2188 - val_loss: 135480.7574 - val_mae: 135480.7500\n",
      "Epoch 6/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 135681.4864 - mae: 135681.4531 - val_loss: 135464.1933 - val_mae: 135464.1875\n",
      "Epoch 7/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 135662.5518 - mae: 135662.5781 - val_loss: 135441.7730 - val_mae: 135441.7812\n",
      "Epoch 8/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 135637.9282 - mae: 135637.9375 - val_loss: 135413.1311 - val_mae: 135413.1250\n",
      "Epoch 9/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 135606.4363 - mae: 135606.4375 - val_loss: 135377.2417 - val_mae: 135377.2656\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 100us/step - loss: 135567.5732 - mae: 135567.5781 - val_loss: 135332.7946 - val_mae: 135332.7656\n",
      "Epoch 11/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 135520.3038 - mae: 135520.2812 - val_loss: 135279.6926 - val_mae: 135279.7031\n",
      "Epoch 12/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 135463.9086 - mae: 135463.9219 - val_loss: 135216.9444 - val_mae: 135216.9219\n",
      "Epoch 13/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 135397.8726 - mae: 135397.8750 - val_loss: 135143.5554 - val_mae: 135143.5625\n",
      "Epoch 14/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 135320.6966 - mae: 135320.7031 - val_loss: 135058.1262 - val_mae: 135058.1406\n",
      "Epoch 15/200\n",
      "1063/1063 [==============================] - 0s 96us/step - loss: 135232.0734 - mae: 135232.0938 - val_loss: 134961.1844 - val_mae: 134961.1875\n",
      "Epoch 16/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 135130.7534 - mae: 135130.7656 - val_loss: 134850.2986 - val_mae: 134850.3125\n",
      "Epoch 17/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 135015.6300 - mae: 135015.5938 - val_loss: 134724.9313 - val_mae: 134724.9375\n",
      "Epoch 18/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 134886.9532 - mae: 134886.9531 - val_loss: 134584.7351 - val_mae: 134584.7500\n",
      "Epoch 19/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 134743.5252 - mae: 134743.5469 - val_loss: 134428.9947 - val_mae: 134429.0000\n",
      "Epoch 20/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 134583.2818 - mae: 134583.2812 - val_loss: 134257.0752 - val_mae: 134257.0781\n",
      "Epoch 21/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 134407.7761 - mae: 134407.7969 - val_loss: 134066.8259 - val_mae: 134066.8438\n",
      "Epoch 22/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 134213.7651 - mae: 134213.8125 - val_loss: 133859.2214 - val_mae: 133859.2344\n",
      "Epoch 23/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 134003.8717 - mae: 134003.8750 - val_loss: 133632.5611 - val_mae: 133632.5469\n",
      "Epoch 24/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 133772.8088 - mae: 133772.8125 - val_loss: 133384.7037 - val_mae: 133384.7031\n",
      "Epoch 25/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 133519.3159 - mae: 133519.3281 - val_loss: 133114.1098 - val_mae: 133114.1094\n",
      "Epoch 26/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 133246.3429 - mae: 133246.3750 - val_loss: 132821.3185 - val_mae: 132821.3125\n",
      "Epoch 27/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 132950.7694 - mae: 132950.7656 - val_loss: 132507.4185 - val_mae: 132507.4219\n",
      "Epoch 28/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 132635.0391 - mae: 132635.0625 - val_loss: 132170.6992 - val_mae: 132170.6719\n",
      "Epoch 29/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 132293.4237 - mae: 132293.4062 - val_loss: 131808.6800 - val_mae: 131808.6562\n",
      "Epoch 30/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 131929.7211 - mae: 131929.7344 - val_loss: 131421.5468 - val_mae: 131421.5625\n",
      "Epoch 31/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 131540.3319 - mae: 131540.3281 - val_loss: 131006.4778 - val_mae: 131006.4531\n",
      "Epoch 32/200\n",
      "1063/1063 [==============================] - 0s 96us/step - loss: 131123.7067 - mae: 131123.7500 - val_loss: 130564.1970 - val_mae: 130564.2031\n",
      "Epoch 33/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 130680.6326 - mae: 130680.6094 - val_loss: 130093.3633 - val_mae: 130093.3672\n",
      "Epoch 34/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 130207.7920 - mae: 130207.7422 - val_loss: 129594.5018 - val_mae: 129594.4844\n",
      "Epoch 35/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 129709.0179 - mae: 129709.0078 - val_loss: 129065.0919 - val_mae: 129065.0859\n",
      "Epoch 36/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 129182.5896 - mae: 129182.5859 - val_loss: 128508.4075 - val_mae: 128508.4062\n",
      "Epoch 37/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 128624.3739 - mae: 128624.3906 - val_loss: 127918.7154 - val_mae: 127918.7266\n",
      "Epoch 38/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 128037.4368 - mae: 128037.4219 - val_loss: 127298.8762 - val_mae: 127298.8828\n",
      "Epoch 39/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 127414.5581 - mae: 127414.5781 - val_loss: 126639.3053 - val_mae: 126639.3125\n",
      "Epoch 40/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 126757.0191 - mae: 126757.0000 - val_loss: 125956.6514 - val_mae: 125956.6328\n",
      "Epoch 41/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 126098.2608 - mae: 126098.2344 - val_loss: 125296.2133 - val_mae: 125296.1953\n",
      "Epoch 42/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 125435.2445 - mae: 125435.2266 - val_loss: 124659.4518 - val_mae: 124659.4219\n",
      "Epoch 43/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 124798.4557 - mae: 124798.4609 - val_loss: 124040.6877 - val_mae: 124040.6797\n",
      "Epoch 44/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 124159.7415 - mae: 124159.7031 - val_loss: 123421.5983 - val_mae: 123421.6172\n",
      "Epoch 45/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 123523.3784 - mae: 123523.3828 - val_loss: 122817.4926 - val_mae: 122817.4766\n",
      "Epoch 46/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 122879.3397 - mae: 122879.3125 - val_loss: 122195.1826 - val_mae: 122195.1875\n",
      "Epoch 47/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 122243.1630 - mae: 122243.1719 - val_loss: 121550.9685 - val_mae: 121550.9609\n",
      "Epoch 48/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 121595.5008 - mae: 121595.4844 - val_loss: 120892.2649 - val_mae: 120892.2812\n",
      "Epoch 49/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 120934.0103 - mae: 120934.0000 - val_loss: 120228.0822 - val_mae: 120228.1094\n",
      "Epoch 50/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 120258.7763 - mae: 120258.8047 - val_loss: 119553.3660 - val_mae: 119553.3672\n",
      "Epoch 51/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 119560.1885 - mae: 119560.1797 - val_loss: 118860.2214 - val_mae: 118860.2266\n",
      "Epoch 52/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 118859.4669 - mae: 118859.4531 - val_loss: 118148.7295 - val_mae: 118148.7031\n",
      "Epoch 53/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 118125.0765 - mae: 118125.0781 - val_loss: 117409.1490 - val_mae: 117409.1562\n",
      "Epoch 54/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 117382.7006 - mae: 117382.7266 - val_loss: 116657.4601 - val_mae: 116657.4688\n",
      "Epoch 55/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 116614.6573 - mae: 116614.6797 - val_loss: 115903.2861 - val_mae: 115903.2891\n",
      "Epoch 56/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 115831.3815 - mae: 115831.3516 - val_loss: 115145.5502 - val_mae: 115145.5703\n",
      "Epoch 57/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 115038.1624 - mae: 115038.1797 - val_loss: 114368.5254 - val_mae: 114368.5234\n",
      "Epoch 58/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 114239.0762 - mae: 114239.0625 - val_loss: 113565.8418 - val_mae: 113565.8281\n",
      "Epoch 59/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 113422.2433 - mae: 113422.2578 - val_loss: 112731.0457 - val_mae: 112731.0547\n",
      "Epoch 60/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 112580.8577 - mae: 112580.8594 - val_loss: 111886.2345 - val_mae: 111886.2500\n",
      "Epoch 61/200\n",
      "1063/1063 [==============================] - 0s 114us/step - loss: 111712.4980 - mae: 111712.5156 - val_loss: 111002.6706 - val_mae: 111002.6406\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 102us/step - loss: 110829.5082 - mae: 110829.5000 - val_loss: 110111.6869 - val_mae: 110111.6719\n",
      "Epoch 63/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 109918.9780 - mae: 109918.9844 - val_loss: 109197.6801 - val_mae: 109197.6953\n",
      "Epoch 64/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 108997.9603 - mae: 108997.9688 - val_loss: 108256.4503 - val_mae: 108256.4531\n",
      "Epoch 65/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 108071.4792 - mae: 108071.4844 - val_loss: 107292.0168 - val_mae: 107292.0000\n",
      "Epoch 66/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 107111.7009 - mae: 107111.7109 - val_loss: 106316.1820 - val_mae: 106316.1797\n",
      "Epoch 67/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 106140.8620 - mae: 106140.8516 - val_loss: 105321.5549 - val_mae: 105321.5469\n",
      "Epoch 68/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 105113.2948 - mae: 105113.2969 - val_loss: 104283.5678 - val_mae: 104283.5781\n",
      "Epoch 69/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 104060.6236 - mae: 104060.6172 - val_loss: 103237.7150 - val_mae: 103237.7188\n",
      "Epoch 70/200\n",
      "1063/1063 [==============================] - 0s 107us/step - loss: 103011.4982 - mae: 103011.4844 - val_loss: 102168.8566 - val_mae: 102168.8438\n",
      "Epoch 71/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 101957.9594 - mae: 101957.9375 - val_loss: 101100.5150 - val_mae: 101100.5078\n",
      "Epoch 72/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 100886.7187 - mae: 100886.7422 - val_loss: 99995.4857 - val_mae: 99995.4922\n",
      "Epoch 73/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 99815.2373 - mae: 99815.2422 - val_loss: 98942.5425 - val_mae: 98942.5312\n",
      "Epoch 74/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 98777.2200 - mae: 98777.2266 - val_loss: 97862.0883 - val_mae: 97862.0938\n",
      "Epoch 75/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 97730.5002 - mae: 97730.5078 - val_loss: 96831.9528 - val_mae: 96831.9531\n",
      "Epoch 76/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 96705.9886 - mae: 96705.9766 - val_loss: 95793.8820 - val_mae: 95793.8750\n",
      "Epoch 77/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 95670.2086 - mae: 95670.1797 - val_loss: 94749.6498 - val_mae: 94749.6406\n",
      "Epoch 78/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 94628.2824 - mae: 94628.2422 - val_loss: 93738.4120 - val_mae: 93738.4062\n",
      "Epoch 79/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 93603.8803 - mae: 93603.8828 - val_loss: 92746.8606 - val_mae: 92746.8672\n",
      "Epoch 80/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 92568.3780 - mae: 92568.3828 - val_loss: 91774.4989 - val_mae: 91774.4922\n",
      "Epoch 81/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 91562.5962 - mae: 91562.6094 - val_loss: 90816.9061 - val_mae: 90816.8984\n",
      "Epoch 82/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 90543.4451 - mae: 90543.4219 - val_loss: 89858.5394 - val_mae: 89858.5391\n",
      "Epoch 83/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 89546.5671 - mae: 89546.5625 - val_loss: 88905.5814 - val_mae: 88905.5781\n",
      "Epoch 84/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 88525.8773 - mae: 88525.8750 - val_loss: 87981.3388 - val_mae: 87981.3438\n",
      "Epoch 85/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 87525.4646 - mae: 87525.4688 - val_loss: 87063.3329 - val_mae: 87063.3438\n",
      "Epoch 86/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 86526.3371 - mae: 86526.3047 - val_loss: 86113.5337 - val_mae: 86113.5312\n",
      "Epoch 87/200\n",
      "1063/1063 [==============================] - 0s 115us/step - loss: 85495.4464 - mae: 85495.4297 - val_loss: 85153.7384 - val_mae: 85153.7422\n",
      "Epoch 88/200\n",
      "1063/1063 [==============================] - 0s 111us/step - loss: 84508.1504 - mae: 84508.1484 - val_loss: 84227.6316 - val_mae: 84227.6406\n",
      "Epoch 89/200\n",
      "1063/1063 [==============================] - 0s 132us/step - loss: 83509.7742 - mae: 83509.7891 - val_loss: 83293.5463 - val_mae: 83293.5391\n",
      "Epoch 90/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 82531.7921 - mae: 82531.8125 - val_loss: 82347.3821 - val_mae: 82347.3828\n",
      "Epoch 91/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 81521.5799 - mae: 81521.5703 - val_loss: 81403.0417 - val_mae: 81403.0469\n",
      "Epoch 92/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 80581.2213 - mae: 80581.2031 - val_loss: 80502.7641 - val_mae: 80502.7500\n",
      "Epoch 93/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 79648.0346 - mae: 79647.9922 - val_loss: 79623.5991 - val_mae: 79623.6016\n",
      "Epoch 94/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 78751.1985 - mae: 78751.2031 - val_loss: 78760.0741 - val_mae: 78760.0703\n",
      "Epoch 95/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 77842.9083 - mae: 77842.9062 - val_loss: 77902.8468 - val_mae: 77902.8438\n",
      "Epoch 96/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 76987.6485 - mae: 76987.6406 - val_loss: 77073.2616 - val_mae: 77073.2422\n",
      "Epoch 97/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 76174.9562 - mae: 76174.9531 - val_loss: 76294.4966 - val_mae: 76294.5156\n",
      "Epoch 98/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 75403.0430 - mae: 75403.0547 - val_loss: 75563.4904 - val_mae: 75563.4922\n",
      "Epoch 99/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 74687.3751 - mae: 74687.3828 - val_loss: 74828.6608 - val_mae: 74828.6562\n",
      "Epoch 100/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 73963.9871 - mae: 73964.0000 - val_loss: 74131.4780 - val_mae: 74131.4766\n",
      "Epoch 101/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 73313.9386 - mae: 73313.9453 - val_loss: 73456.8555 - val_mae: 73456.8672\n",
      "Epoch 102/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 72669.3393 - mae: 72669.3359 - val_loss: 72811.2044 - val_mae: 72811.2031\n",
      "Epoch 103/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 72063.4482 - mae: 72063.4453 - val_loss: 72184.7412 - val_mae: 72184.7344\n",
      "Epoch 104/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 71454.0394 - mae: 71454.0547 - val_loss: 71534.9865 - val_mae: 71534.9766\n",
      "Epoch 105/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 70847.5752 - mae: 70847.5547 - val_loss: 70940.1698 - val_mae: 70940.1875\n",
      "Epoch 106/200\n",
      "1063/1063 [==============================] - 0s 111us/step - loss: 70277.4488 - mae: 70277.4375 - val_loss: 70353.8408 - val_mae: 70353.8438\n",
      "Epoch 107/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 69727.7906 - mae: 69727.7969 - val_loss: 69781.3727 - val_mae: 69781.3672\n",
      "Epoch 108/200\n",
      "1063/1063 [==============================] - 0s 112us/step - loss: 69199.2303 - mae: 69199.2344 - val_loss: 69233.8986 - val_mae: 69233.9062\n",
      "Epoch 109/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 68670.3231 - mae: 68670.3281 - val_loss: 68732.3663 - val_mae: 68732.3594\n",
      "Epoch 110/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 68182.4529 - mae: 68182.4609 - val_loss: 68235.3620 - val_mae: 68235.3594\n",
      "Epoch 111/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 67696.2600 - mae: 67696.2500 - val_loss: 67761.5932 - val_mae: 67761.6016\n",
      "Epoch 112/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 67238.6304 - mae: 67238.6328 - val_loss: 67336.1527 - val_mae: 67336.1562\n",
      "Epoch 113/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 66826.2709 - mae: 66826.2812 - val_loss: 66961.4684 - val_mae: 66961.4766\n",
      "Epoch 114/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 66441.5040 - mae: 66441.5000 - val_loss: 66632.5397 - val_mae: 66632.5391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 66084.9893 - mae: 66084.9922 - val_loss: 66321.6547 - val_mae: 66321.6641\n",
      "Epoch 116/200\n",
      "1063/1063 [==============================] - 0s 108us/step - loss: 65737.5569 - mae: 65737.5781 - val_loss: 66010.2289 - val_mae: 66010.2266\n",
      "Epoch 117/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 65420.0708 - mae: 65420.0859 - val_loss: 65725.6119 - val_mae: 65725.6016\n",
      "Epoch 118/200\n",
      "1063/1063 [==============================] - 0s 111us/step - loss: 65099.3573 - mae: 65099.3711 - val_loss: 65450.3989 - val_mae: 65450.3906\n",
      "Epoch 119/200\n",
      "1063/1063 [==============================] - 0s 95us/step - loss: 64820.5814 - mae: 64820.5742 - val_loss: 65219.0960 - val_mae: 65219.0859\n",
      "Epoch 120/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 64581.2140 - mae: 64581.2070 - val_loss: 65030.7545 - val_mae: 65030.7383\n",
      "Epoch 121/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 64353.9890 - mae: 64353.9883 - val_loss: 64835.9165 - val_mae: 64835.9023\n",
      "Epoch 122/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 64128.4902 - mae: 64128.4883 - val_loss: 64649.6256 - val_mae: 64649.6289\n",
      "Epoch 123/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 63906.4816 - mae: 63906.4766 - val_loss: 64465.8787 - val_mae: 64465.8945\n",
      "Epoch 124/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 63677.9201 - mae: 63677.9141 - val_loss: 64292.1217 - val_mae: 64292.1250\n",
      "Epoch 125/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 63456.8616 - mae: 63456.8672 - val_loss: 64142.7124 - val_mae: 64142.7188\n",
      "Epoch 126/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 63264.0482 - mae: 63264.0391 - val_loss: 64016.1819 - val_mae: 64016.1758\n",
      "Epoch 127/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 63087.7966 - mae: 63087.7930 - val_loss: 63893.4352 - val_mae: 63893.4219\n",
      "Epoch 128/200\n",
      "1063/1063 [==============================] - 0s 110us/step - loss: 62924.9447 - mae: 62924.9336 - val_loss: 63783.8789 - val_mae: 63783.8750\n",
      "Epoch 129/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 62787.0897 - mae: 62787.0820 - val_loss: 63697.7819 - val_mae: 63697.7734\n",
      "Epoch 130/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 62661.9282 - mae: 62661.9297 - val_loss: 63610.9979 - val_mae: 63610.9961\n",
      "Epoch 131/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 62545.3562 - mae: 62545.3633 - val_loss: 63532.8763 - val_mae: 63532.8867\n",
      "Epoch 132/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 62430.2990 - mae: 62430.2812 - val_loss: 63442.4816 - val_mae: 63442.4805\n",
      "Epoch 133/200\n",
      "1063/1063 [==============================] - 0s 115us/step - loss: 62313.5420 - mae: 62313.5430 - val_loss: 63354.5491 - val_mae: 63354.5547\n",
      "Epoch 134/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 62202.8962 - mae: 62202.8945 - val_loss: 63272.5319 - val_mae: 63272.5312\n",
      "Epoch 135/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 62097.8101 - mae: 62097.8047 - val_loss: 63185.5167 - val_mae: 63185.5156\n",
      "Epoch 136/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 62006.6854 - mae: 62006.6953 - val_loss: 63112.2110 - val_mae: 63112.2148\n",
      "Epoch 137/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 61928.1008 - mae: 61928.1016 - val_loss: 63040.2808 - val_mae: 63040.2891\n",
      "Epoch 138/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 61856.9544 - mae: 61856.9492 - val_loss: 62974.8256 - val_mae: 62974.8438\n",
      "Epoch 139/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 61788.5214 - mae: 61788.5195 - val_loss: 62903.5672 - val_mae: 62903.5586\n",
      "Epoch 140/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 61726.7720 - mae: 61726.7891 - val_loss: 62839.5064 - val_mae: 62839.4961\n",
      "Epoch 141/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 61666.1344 - mae: 61666.1367 - val_loss: 62770.8687 - val_mae: 62770.8711\n",
      "Epoch 142/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 61604.8106 - mae: 61604.8008 - val_loss: 62709.4259 - val_mae: 62709.4297\n",
      "Epoch 143/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 61543.5627 - mae: 61543.5547 - val_loss: 62651.7253 - val_mae: 62651.7344\n",
      "Epoch 144/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 61481.7794 - mae: 61481.7852 - val_loss: 62594.2380 - val_mae: 62594.2383\n",
      "Epoch 145/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 61424.9809 - mae: 61424.9844 - val_loss: 62539.5311 - val_mae: 62539.5352\n",
      "Epoch 146/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 61371.5051 - mae: 61371.4805 - val_loss: 62487.1910 - val_mae: 62487.1875\n",
      "Epoch 147/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 61321.1661 - mae: 61321.1680 - val_loss: 62435.7837 - val_mae: 62435.7734\n",
      "Epoch 148/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 61276.0151 - mae: 61276.0078 - val_loss: 62385.6006 - val_mae: 62385.5977\n",
      "Epoch 149/200\n",
      "1063/1063 [==============================] - 0s 109us/step - loss: 61234.9872 - mae: 61234.9922 - val_loss: 62342.4192 - val_mae: 62342.4219\n",
      "Epoch 150/200\n",
      "1063/1063 [==============================] - 0s 111us/step - loss: 61194.3975 - mae: 61194.4023 - val_loss: 62299.5258 - val_mae: 62299.5312\n",
      "Epoch 151/200\n",
      "1063/1063 [==============================] - 0s 108us/step - loss: 61153.3139 - mae: 61153.3438 - val_loss: 62260.7794 - val_mae: 62260.7812\n",
      "Epoch 152/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 61114.1744 - mae: 61114.1836 - val_loss: 62218.7431 - val_mae: 62218.7461\n",
      "Epoch 153/200\n",
      "1063/1063 [==============================] - 0s 109us/step - loss: 61075.1625 - mae: 61075.1758 - val_loss: 62178.2634 - val_mae: 62178.2617\n",
      "Epoch 154/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 61035.1885 - mae: 61035.1992 - val_loss: 62147.4693 - val_mae: 62147.4727\n",
      "Epoch 155/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60999.1228 - mae: 60999.1406 - val_loss: 62117.8508 - val_mae: 62117.8516\n",
      "Epoch 156/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60960.6179 - mae: 60960.6289 - val_loss: 62083.4304 - val_mae: 62083.4219\n",
      "Epoch 157/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 60921.9124 - mae: 60921.9023 - val_loss: 62053.8365 - val_mae: 62053.8359\n",
      "Epoch 158/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60883.0557 - mae: 60883.0430 - val_loss: 62024.6975 - val_mae: 62024.6953\n",
      "Epoch 159/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60845.3916 - mae: 60845.3867 - val_loss: 61992.6728 - val_mae: 61992.6719\n",
      "Epoch 160/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 60807.2241 - mae: 60807.2227 - val_loss: 61964.2280 - val_mae: 61964.2422\n",
      "Epoch 161/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60772.9321 - mae: 60772.9219 - val_loss: 61931.4812 - val_mae: 61931.4727\n",
      "Epoch 162/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60734.9824 - mae: 60734.9688 - val_loss: 61901.2461 - val_mae: 61901.2539\n",
      "Epoch 163/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 60699.0077 - mae: 60699.0156 - val_loss: 61873.3595 - val_mae: 61873.3516\n",
      "Epoch 164/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 60665.1849 - mae: 60665.1836 - val_loss: 61845.0456 - val_mae: 61845.0547\n",
      "Epoch 165/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 60634.3823 - mae: 60634.4062 - val_loss: 61818.5883 - val_mae: 61818.5859\n",
      "Epoch 166/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 60607.5958 - mae: 60607.5977 - val_loss: 61792.3219 - val_mae: 61792.3281\n",
      "Epoch 167/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60582.9285 - mae: 60582.9336 - val_loss: 61768.6810 - val_mae: 61768.6797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 60558.8441 - mae: 60558.8242 - val_loss: 61741.6651 - val_mae: 61741.6602\n",
      "Epoch 169/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60537.2119 - mae: 60537.2188 - val_loss: 61719.4233 - val_mae: 61719.4062\n",
      "Epoch 170/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 60512.6975 - mae: 60512.6914 - val_loss: 61692.4602 - val_mae: 61692.4531\n",
      "Epoch 171/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60488.7341 - mae: 60488.7344 - val_loss: 61667.4586 - val_mae: 61667.4688\n",
      "Epoch 172/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60465.5232 - mae: 60465.5117 - val_loss: 61647.8780 - val_mae: 61647.8750\n",
      "Epoch 173/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 60442.2854 - mae: 60442.2891 - val_loss: 61629.4133 - val_mae: 61629.4180\n",
      "Epoch 174/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60419.5681 - mae: 60419.5781 - val_loss: 61612.9198 - val_mae: 61612.9141\n",
      "Epoch 175/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60398.1810 - mae: 60398.1875 - val_loss: 61593.1983 - val_mae: 61593.2031\n",
      "Epoch 176/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 60379.5614 - mae: 60379.5664 - val_loss: 61575.4860 - val_mae: 61575.4922\n",
      "Epoch 177/200\n",
      "1063/1063 [==============================] - 0s 96us/step - loss: 60362.1342 - mae: 60362.1328 - val_loss: 61558.9253 - val_mae: 61558.9258\n",
      "Epoch 178/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60345.0367 - mae: 60345.0352 - val_loss: 61542.4557 - val_mae: 61542.4453\n",
      "Epoch 179/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 60327.6561 - mae: 60327.6445 - val_loss: 61523.0945 - val_mae: 61523.0859\n",
      "Epoch 180/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 60309.0249 - mae: 60309.0234 - val_loss: 61505.3932 - val_mae: 61505.3789\n",
      "Epoch 181/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 60290.2264 - mae: 60290.2148 - val_loss: 61488.8026 - val_mae: 61488.7969\n",
      "Epoch 182/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 60272.7648 - mae: 60272.7656 - val_loss: 61470.3023 - val_mae: 61470.3164\n",
      "Epoch 183/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60255.2346 - mae: 60255.2383 - val_loss: 61451.1208 - val_mae: 61451.1133\n",
      "Epoch 184/200\n",
      "1063/1063 [==============================] - 0s 109us/step - loss: 60239.9298 - mae: 60239.9258 - val_loss: 61434.9779 - val_mae: 61434.9766\n",
      "Epoch 185/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60225.9680 - mae: 60225.9688 - val_loss: 61421.3174 - val_mae: 61421.3164\n",
      "Epoch 186/200\n",
      "1063/1063 [==============================] - 0s 113us/step - loss: 60210.0135 - mae: 60210.0078 - val_loss: 61407.4839 - val_mae: 61407.4727\n",
      "Epoch 187/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 60193.3727 - mae: 60193.3711 - val_loss: 61392.6438 - val_mae: 61392.6445\n",
      "Epoch 188/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60178.9229 - mae: 60178.9336 - val_loss: 61378.3215 - val_mae: 61378.3281\n",
      "Epoch 189/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 60163.7411 - mae: 60163.7422 - val_loss: 61362.7433 - val_mae: 61362.7500\n",
      "Epoch 190/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60150.9432 - mae: 60150.9609 - val_loss: 61346.8588 - val_mae: 61346.8594\n",
      "Epoch 191/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60136.3872 - mae: 60136.3867 - val_loss: 61332.1497 - val_mae: 61332.1484\n",
      "Epoch 192/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60123.1672 - mae: 60123.1719 - val_loss: 61319.0065 - val_mae: 61319.0039\n",
      "Epoch 193/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 60107.5117 - mae: 60107.5195 - val_loss: 61303.9389 - val_mae: 61303.9375\n",
      "Epoch 194/200\n",
      "1063/1063 [==============================] - 0s 109us/step - loss: 60094.2622 - mae: 60094.2695 - val_loss: 61290.1315 - val_mae: 61290.1250\n",
      "Epoch 195/200\n",
      "1063/1063 [==============================] - 0s 107us/step - loss: 60080.8952 - mae: 60080.8867 - val_loss: 61274.1473 - val_mae: 61274.1484\n",
      "Epoch 196/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 60066.5343 - mae: 60066.5391 - val_loss: 61258.5857 - val_mae: 61258.5781\n",
      "Epoch 197/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 60055.2424 - mae: 60055.2617 - val_loss: 61240.9699 - val_mae: 61240.9844\n",
      "Epoch 198/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 60040.6507 - mae: 60040.6562 - val_loss: 61224.6185 - val_mae: 61224.6211\n",
      "Epoch 199/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60028.4711 - mae: 60028.4609 - val_loss: 61204.9666 - val_mae: 61204.9727\n",
      "Epoch 200/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60014.0468 - mae: 60014.0547 - val_loss: 61188.8333 - val_mae: 61188.8359\n",
      "processing fold # 2\n",
      "Train on 1063 samples, validate on 354 samples\n",
      "Epoch 1/200\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 134988.1216 - mae: 134988.1250 - val_loss: 137697.7395 - val_mae: 137697.7344\n",
      "Epoch 2/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 134985.0343 - mae: 134985.0156 - val_loss: 137693.6887 - val_mae: 137693.7031\n",
      "Epoch 3/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 134979.6366 - mae: 134979.6094 - val_loss: 137686.8681 - val_mae: 137686.8750\n",
      "Epoch 4/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 134970.7824 - mae: 134970.7188 - val_loss: 137676.1239 - val_mae: 137676.1094\n",
      "Epoch 5/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 134957.6045 - mae: 134957.6094 - val_loss: 137660.7340 - val_mae: 137660.7500\n",
      "Epoch 6/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 134939.0808 - mae: 134939.0625 - val_loss: 137639.8836 - val_mae: 137639.8750\n",
      "Epoch 7/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 134914.3566 - mae: 134914.3750 - val_loss: 137612.5917 - val_mae: 137612.5938\n",
      "Epoch 8/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 134883.0511 - mae: 134883.0469 - val_loss: 137577.9752 - val_mae: 137577.9531\n",
      "Epoch 9/200\n",
      "1063/1063 [==============================] - 0s 107us/step - loss: 134843.3301 - mae: 134843.3750 - val_loss: 137535.5636 - val_mae: 137535.5469\n",
      "Epoch 10/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 134794.9385 - mae: 134794.9219 - val_loss: 137484.0983 - val_mae: 137484.1094\n",
      "Epoch 11/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 134737.5687 - mae: 134737.5312 - val_loss: 137423.2551 - val_mae: 137423.2344\n",
      "Epoch 12/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 134669.9300 - mae: 134669.9219 - val_loss: 137351.8973 - val_mae: 137351.8906\n",
      "Epoch 13/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 134589.8696 - mae: 134589.8594 - val_loss: 137268.6142 - val_mae: 137268.6094\n",
      "Epoch 14/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 134497.7205 - mae: 134497.7500 - val_loss: 137173.1989 - val_mae: 137173.2031\n",
      "Epoch 15/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 134392.5393 - mae: 134392.5625 - val_loss: 137064.6474 - val_mae: 137064.6562\n",
      "Epoch 16/200\n",
      "1063/1063 [==============================] - 0s 117us/step - loss: 134273.9158 - mae: 134273.9375 - val_loss: 136942.0615 - val_mae: 136942.0469\n",
      "Epoch 17/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 134140.1312 - mae: 134140.1562 - val_loss: 136805.1241 - val_mae: 136805.1250\n",
      "Epoch 18/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 133989.5726 - mae: 133989.5469 - val_loss: 136651.2985 - val_mae: 136651.2812\n",
      "Epoch 19/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 133822.5241 - mae: 133822.5469 - val_loss: 136482.1680 - val_mae: 136482.1719\n",
      "Epoch 20/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 104us/step - loss: 133638.6790 - mae: 133638.6875 - val_loss: 136295.0583 - val_mae: 136295.0625\n",
      "Epoch 21/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 133433.5138 - mae: 133433.5312 - val_loss: 136088.1007 - val_mae: 136088.1094\n",
      "Epoch 22/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 133211.7324 - mae: 133211.7188 - val_loss: 135864.1484 - val_mae: 135864.1406\n",
      "Epoch 23/200\n",
      "1063/1063 [==============================] - 0s 107us/step - loss: 132967.3252 - mae: 132967.3750 - val_loss: 135618.2391 - val_mae: 135618.2188\n",
      "Epoch 24/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 132704.6926 - mae: 132704.6875 - val_loss: 135354.4714 - val_mae: 135354.4688\n",
      "Epoch 25/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 132412.7605 - mae: 132412.7969 - val_loss: 135066.2254 - val_mae: 135066.2188\n",
      "Epoch 26/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 132103.1555 - mae: 132103.1719 - val_loss: 134757.9835 - val_mae: 134757.9688\n",
      "Epoch 27/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 131768.8277 - mae: 131768.8438 - val_loss: 134424.7112 - val_mae: 134424.7188\n",
      "Epoch 28/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 131408.4209 - mae: 131408.4375 - val_loss: 134065.1771 - val_mae: 134065.1562\n",
      "Epoch 29/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 131021.6369 - mae: 131021.6797 - val_loss: 133683.5287 - val_mae: 133683.5312\n",
      "Epoch 30/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 130605.9869 - mae: 130606.0312 - val_loss: 133273.0826 - val_mae: 133273.0781\n",
      "Epoch 31/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 130165.9543 - mae: 130165.9297 - val_loss: 132836.1642 - val_mae: 132836.1719\n",
      "Epoch 32/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 129693.9652 - mae: 129693.9688 - val_loss: 132369.4142 - val_mae: 132369.4062\n",
      "Epoch 33/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 129196.3103 - mae: 129196.2656 - val_loss: 131877.7837 - val_mae: 131877.7812\n",
      "Epoch 34/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 128666.8803 - mae: 128666.8672 - val_loss: 131355.5848 - val_mae: 131355.5938\n",
      "Epoch 35/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 128100.6516 - mae: 128100.6484 - val_loss: 130793.5826 - val_mae: 130793.5703\n",
      "Epoch 36/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 127495.1092 - mae: 127495.1172 - val_loss: 130202.6799 - val_mae: 130202.6641\n",
      "Epoch 37/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 126872.0272 - mae: 126872.0391 - val_loss: 129589.2778 - val_mae: 129589.2734\n",
      "Epoch 38/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 126249.2837 - mae: 126249.2891 - val_loss: 128972.1287 - val_mae: 128972.1328\n",
      "Epoch 39/200\n",
      "1063/1063 [==============================] - 0s 108us/step - loss: 125655.8412 - mae: 125655.8438 - val_loss: 128390.1069 - val_mae: 128390.0938\n",
      "Epoch 40/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 125081.1160 - mae: 125081.1562 - val_loss: 127824.2353 - val_mae: 127824.2344\n",
      "Epoch 41/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 124521.7931 - mae: 124521.7969 - val_loss: 127281.1514 - val_mae: 127281.1641\n",
      "Epoch 42/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 123966.1679 - mae: 123966.1719 - val_loss: 126742.9086 - val_mae: 126742.9141\n",
      "Epoch 43/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 123431.5204 - mae: 123431.5156 - val_loss: 126199.5166 - val_mae: 126199.5000\n",
      "Epoch 44/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 122894.5886 - mae: 122894.5859 - val_loss: 125654.7841 - val_mae: 125654.8125\n",
      "Epoch 45/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 122351.9293 - mae: 122351.9531 - val_loss: 125099.7635 - val_mae: 125099.7734\n",
      "Epoch 46/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 121792.2384 - mae: 121792.2344 - val_loss: 124532.0966 - val_mae: 124532.0938\n",
      "Epoch 47/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 121205.5590 - mae: 121205.5625 - val_loss: 123952.2534 - val_mae: 123952.2812\n",
      "Epoch 48/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 120605.8443 - mae: 120605.8438 - val_loss: 123355.5576 - val_mae: 123355.5469\n",
      "Epoch 49/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 120004.3653 - mae: 120004.3359 - val_loss: 122753.9868 - val_mae: 122753.9688\n",
      "Epoch 50/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 119372.6450 - mae: 119372.6328 - val_loss: 122124.8039 - val_mae: 122124.7891\n",
      "Epoch 51/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 118736.9956 - mae: 118736.9922 - val_loss: 121477.8270 - val_mae: 121477.8438\n",
      "Epoch 52/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 118084.7938 - mae: 118084.8047 - val_loss: 120846.2474 - val_mae: 120846.2578\n",
      "Epoch 53/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 117428.2854 - mae: 117428.2812 - val_loss: 120202.3967 - val_mae: 120202.3828\n",
      "Epoch 54/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 116766.3424 - mae: 116766.3438 - val_loss: 119561.8265 - val_mae: 119561.8281\n",
      "Epoch 55/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 116086.6257 - mae: 116086.6172 - val_loss: 118901.5177 - val_mae: 118901.5234\n",
      "Epoch 56/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 115404.0398 - mae: 115404.0312 - val_loss: 118226.5638 - val_mae: 118226.6016\n",
      "Epoch 57/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 114694.7410 - mae: 114694.7344 - val_loss: 117523.5279 - val_mae: 117523.5391\n",
      "Epoch 58/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 113979.8600 - mae: 113979.8672 - val_loss: 116801.1665 - val_mae: 116801.1562\n",
      "Epoch 59/200\n",
      "1063/1063 [==============================] - 0s 96us/step - loss: 113249.9253 - mae: 113249.9219 - val_loss: 116069.9943 - val_mae: 116070.0078\n",
      "Epoch 60/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 112485.6745 - mae: 112485.6953 - val_loss: 115299.6048 - val_mae: 115299.6016\n",
      "Epoch 61/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 111686.2879 - mae: 111686.2734 - val_loss: 114513.5479 - val_mae: 114513.5625\n",
      "Epoch 62/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 110864.9441 - mae: 110864.9219 - val_loss: 113703.4431 - val_mae: 113703.4609\n",
      "Epoch 63/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 110041.7940 - mae: 110041.8203 - val_loss: 112872.8163 - val_mae: 112872.8047\n",
      "Epoch 64/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 109185.1221 - mae: 109185.1016 - val_loss: 112035.8095 - val_mae: 112035.8281\n",
      "Epoch 65/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 108330.5199 - mae: 108330.5391 - val_loss: 111197.7076 - val_mae: 111197.7031\n",
      "Epoch 66/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 107484.8998 - mae: 107484.8906 - val_loss: 110366.5519 - val_mae: 110366.5547\n",
      "Epoch 67/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 106628.8561 - mae: 106628.8516 - val_loss: 109564.3167 - val_mae: 109564.3047\n",
      "Epoch 68/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 105785.4084 - mae: 105785.4062 - val_loss: 108780.0340 - val_mae: 108780.0078\n",
      "Epoch 69/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 104952.7305 - mae: 104952.7266 - val_loss: 107988.1088 - val_mae: 107988.0938\n",
      "Epoch 70/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 104089.9933 - mae: 104090.0391 - val_loss: 107184.9943 - val_mae: 107184.9688\n",
      "Epoch 71/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 103245.5470 - mae: 103245.5391 - val_loss: 106403.6100 - val_mae: 106403.6016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 102408.1212 - mae: 102408.0938 - val_loss: 105606.8984 - val_mae: 105606.8906\n",
      "Epoch 73/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 101595.6732 - mae: 101595.6484 - val_loss: 104845.5563 - val_mae: 104845.5703\n",
      "Epoch 74/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 100790.3796 - mae: 100790.3828 - val_loss: 104039.0955 - val_mae: 104039.1094\n",
      "Epoch 75/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 99962.7648 - mae: 99962.7812 - val_loss: 103241.1232 - val_mae: 103241.1328\n",
      "Epoch 76/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 99144.7195 - mae: 99144.7266 - val_loss: 102426.5785 - val_mae: 102426.5781\n",
      "Epoch 77/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 98305.8809 - mae: 98305.8516 - val_loss: 101592.0985 - val_mae: 101592.1016\n",
      "Epoch 78/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 97485.7344 - mae: 97485.6719 - val_loss: 100742.3795 - val_mae: 100742.3750\n",
      "Epoch 79/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 96629.3829 - mae: 96629.3828 - val_loss: 99900.3805 - val_mae: 99900.3828\n",
      "Epoch 80/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 95787.7909 - mae: 95787.8047 - val_loss: 99063.2007 - val_mae: 99063.1953\n",
      "Epoch 81/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 94951.4568 - mae: 94951.4766 - val_loss: 98242.9554 - val_mae: 98242.9844\n",
      "Epoch 82/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 94116.4888 - mae: 94116.4766 - val_loss: 97418.8266 - val_mae: 97418.8281\n",
      "Epoch 83/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 93309.8281 - mae: 93309.8281 - val_loss: 96571.1623 - val_mae: 96571.1641\n",
      "Epoch 84/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 92484.8766 - mae: 92484.8672 - val_loss: 95709.3577 - val_mae: 95709.3594\n",
      "Epoch 85/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 91646.0479 - mae: 91646.0703 - val_loss: 94836.8373 - val_mae: 94836.8438\n",
      "Epoch 86/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 90823.5361 - mae: 90823.5234 - val_loss: 93969.7099 - val_mae: 93969.7031\n",
      "Epoch 87/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 90023.9539 - mae: 90023.9453 - val_loss: 93116.6835 - val_mae: 93116.6875\n",
      "Epoch 88/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 89230.6396 - mae: 89230.6406 - val_loss: 92242.5395 - val_mae: 92242.5391\n",
      "Epoch 89/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 88436.8859 - mae: 88436.8906 - val_loss: 91380.3215 - val_mae: 91380.3203\n",
      "Epoch 90/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 87621.7244 - mae: 87621.7266 - val_loss: 90501.1724 - val_mae: 90501.1719\n",
      "Epoch 91/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 86799.5237 - mae: 86799.5078 - val_loss: 89618.3465 - val_mae: 89618.3594\n",
      "Epoch 92/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 85975.8866 - mae: 85975.8984 - val_loss: 88734.4050 - val_mae: 88734.3984\n",
      "Epoch 93/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 85186.8568 - mae: 85186.8359 - val_loss: 87835.2372 - val_mae: 87835.2188\n",
      "Epoch 94/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 84372.2037 - mae: 84372.2031 - val_loss: 86938.1961 - val_mae: 86938.1953\n",
      "Epoch 95/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 83577.7275 - mae: 83577.7266 - val_loss: 86057.6124 - val_mae: 86057.6094\n",
      "Epoch 96/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 82798.0557 - mae: 82798.0547 - val_loss: 85166.4725 - val_mae: 85166.4844\n",
      "Epoch 97/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 82022.9031 - mae: 82022.8984 - val_loss: 84333.3273 - val_mae: 84333.3125\n",
      "Epoch 98/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 81249.7604 - mae: 81249.7656 - val_loss: 83487.4421 - val_mae: 83487.4453\n",
      "Epoch 99/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 80495.5968 - mae: 80495.6016 - val_loss: 82680.0627 - val_mae: 82680.0703\n",
      "Epoch 100/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 79751.8643 - mae: 79751.8516 - val_loss: 81862.3063 - val_mae: 81862.3125\n",
      "Epoch 101/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 79009.1332 - mae: 79009.1406 - val_loss: 81075.3155 - val_mae: 81075.3125\n",
      "Epoch 102/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 78281.3913 - mae: 78281.3828 - val_loss: 80321.9720 - val_mae: 80321.9688\n",
      "Epoch 103/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 77578.1812 - mae: 77578.1797 - val_loss: 79632.3625 - val_mae: 79632.3672\n",
      "Epoch 104/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 76917.8990 - mae: 76917.8828 - val_loss: 78947.1167 - val_mae: 78947.1094\n",
      "Epoch 105/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 76237.3186 - mae: 76237.3047 - val_loss: 78271.7801 - val_mae: 78271.7891\n",
      "Epoch 106/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 75593.5827 - mae: 75593.5703 - val_loss: 77613.8402 - val_mae: 77613.8359\n",
      "Epoch 107/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 74994.4932 - mae: 74994.5000 - val_loss: 76973.5612 - val_mae: 76973.5625\n",
      "Epoch 108/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 74426.1775 - mae: 74426.1641 - val_loss: 76350.2832 - val_mae: 76350.2656\n",
      "Epoch 109/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 73890.6165 - mae: 73890.6094 - val_loss: 75734.7137 - val_mae: 75734.7109\n",
      "Epoch 110/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 73340.9014 - mae: 73340.8906 - val_loss: 75132.9730 - val_mae: 75132.9688\n",
      "Epoch 111/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 72821.8555 - mae: 72821.8594 - val_loss: 74566.9188 - val_mae: 74566.9219\n",
      "Epoch 112/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 72314.1837 - mae: 72314.2031 - val_loss: 73953.1347 - val_mae: 73953.1562\n",
      "Epoch 113/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 71790.6492 - mae: 71790.6406 - val_loss: 73367.8293 - val_mae: 73367.8281\n",
      "Epoch 114/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 71302.5213 - mae: 71302.5156 - val_loss: 72794.7047 - val_mae: 72794.7188\n",
      "Epoch 115/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 70833.3531 - mae: 70833.3438 - val_loss: 72269.5246 - val_mae: 72269.5234\n",
      "Epoch 116/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 70386.0413 - mae: 70386.0391 - val_loss: 71748.8438 - val_mae: 71748.8516\n",
      "Epoch 117/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 69954.8015 - mae: 69954.7891 - val_loss: 71246.9576 - val_mae: 71246.9531\n",
      "Epoch 118/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 69531.1772 - mae: 69531.1719 - val_loss: 70746.9822 - val_mae: 70746.9766\n",
      "Epoch 119/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 69113.9419 - mae: 69113.9766 - val_loss: 70260.0194 - val_mae: 70260.0234\n",
      "Epoch 120/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 68711.9965 - mae: 68711.9844 - val_loss: 69789.2578 - val_mae: 69789.2656\n",
      "Epoch 121/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 68350.5419 - mae: 68350.5391 - val_loss: 69375.5776 - val_mae: 69375.5703\n",
      "Epoch 122/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 68000.6380 - mae: 68000.6406 - val_loss: 68940.5233 - val_mae: 68940.5234\n",
      "Epoch 123/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 67664.6786 - mae: 67664.6875 - val_loss: 68532.6218 - val_mae: 68532.6250\n",
      "Epoch 124/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 67328.6571 - mae: 67328.6562 - val_loss: 68112.4884 - val_mae: 68112.4922\n",
      "Epoch 125/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 99us/step - loss: 67000.9936 - mae: 67000.9922 - val_loss: 67719.2255 - val_mae: 67719.2109\n",
      "Epoch 126/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 66695.7969 - mae: 66695.8047 - val_loss: 67350.1211 - val_mae: 67350.1172\n",
      "Epoch 127/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 66428.2257 - mae: 66428.2188 - val_loss: 67027.9829 - val_mae: 67027.9844\n",
      "Epoch 128/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 66164.6727 - mae: 66164.6484 - val_loss: 66714.2903 - val_mae: 66714.2812\n",
      "Epoch 129/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 65912.7671 - mae: 65912.7578 - val_loss: 66405.0143 - val_mae: 66405.0234\n",
      "Epoch 130/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 65694.4863 - mae: 65694.4766 - val_loss: 66136.6069 - val_mae: 66136.6016\n",
      "Epoch 131/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 65493.4608 - mae: 65493.4570 - val_loss: 65878.2654 - val_mae: 65878.2578\n",
      "Epoch 132/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 65294.1113 - mae: 65294.1016 - val_loss: 65592.7743 - val_mae: 65592.7891\n",
      "Epoch 133/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 65091.7877 - mae: 65091.7930 - val_loss: 65325.5748 - val_mae: 65325.5781\n",
      "Epoch 134/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 64910.3332 - mae: 64910.3438 - val_loss: 65079.3751 - val_mae: 65079.3828\n",
      "Epoch 135/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 64745.0067 - mae: 64745.0000 - val_loss: 64863.8580 - val_mae: 64863.8477\n",
      "Epoch 136/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 64591.1918 - mae: 64591.1797 - val_loss: 64653.2456 - val_mae: 64653.2500\n",
      "Epoch 137/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 64427.5110 - mae: 64427.5312 - val_loss: 64448.0392 - val_mae: 64448.0391\n",
      "Epoch 138/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 64280.5194 - mae: 64280.5039 - val_loss: 64261.8857 - val_mae: 64261.8945\n",
      "Epoch 139/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 64134.6141 - mae: 64134.6094 - val_loss: 64079.7301 - val_mae: 64079.7227\n",
      "Epoch 140/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 64001.7445 - mae: 64001.7461 - val_loss: 63903.3888 - val_mae: 63903.3789\n",
      "Epoch 141/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 63873.3385 - mae: 63873.3398 - val_loss: 63729.5033 - val_mae: 63729.5039\n",
      "Epoch 142/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 63747.2579 - mae: 63747.2422 - val_loss: 63559.1824 - val_mae: 63559.1875\n",
      "Epoch 143/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 63626.5886 - mae: 63626.5898 - val_loss: 63397.4927 - val_mae: 63397.4922\n",
      "Epoch 144/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 63502.7826 - mae: 63502.7969 - val_loss: 63244.0583 - val_mae: 63244.0664\n",
      "Epoch 145/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 63379.3104 - mae: 63379.3203 - val_loss: 63084.6103 - val_mae: 63084.6055\n",
      "Epoch 146/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 63256.7232 - mae: 63256.7383 - val_loss: 62919.9286 - val_mae: 62919.9258\n",
      "Epoch 147/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 63146.1882 - mae: 63146.1992 - val_loss: 62773.7021 - val_mae: 62773.7070\n",
      "Epoch 148/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 63040.4500 - mae: 63040.4414 - val_loss: 62629.8032 - val_mae: 62629.8008\n",
      "Epoch 149/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 62943.5430 - mae: 62943.5469 - val_loss: 62496.5487 - val_mae: 62496.5469\n",
      "Epoch 150/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 62851.6325 - mae: 62851.6211 - val_loss: 62368.2457 - val_mae: 62368.2422\n",
      "Epoch 151/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 62756.7136 - mae: 62756.7227 - val_loss: 62234.0023 - val_mae: 62234.0039\n",
      "Epoch 152/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 62657.2740 - mae: 62657.2852 - val_loss: 62102.1571 - val_mae: 62102.1641\n",
      "Epoch 153/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 62568.4981 - mae: 62568.4805 - val_loss: 61991.5239 - val_mae: 61991.5195\n",
      "Epoch 154/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 62479.4291 - mae: 62479.4375 - val_loss: 61870.4256 - val_mae: 61870.4180\n",
      "Epoch 155/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 62391.3743 - mae: 62391.3789 - val_loss: 61756.7750 - val_mae: 61756.7617\n",
      "Epoch 156/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 62305.7220 - mae: 62305.7188 - val_loss: 61641.2590 - val_mae: 61641.2539\n",
      "Epoch 157/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 62213.4310 - mae: 62213.4297 - val_loss: 61515.6802 - val_mae: 61515.6719\n",
      "Epoch 158/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 62129.9993 - mae: 62130.0000 - val_loss: 61412.5500 - val_mae: 61412.5469\n",
      "Epoch 159/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 62043.4150 - mae: 62043.4180 - val_loss: 61307.0901 - val_mae: 61307.0859\n",
      "Epoch 160/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 61957.3826 - mae: 61957.3945 - val_loss: 61238.4142 - val_mae: 61238.4180\n",
      "Epoch 161/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 61883.8891 - mae: 61883.8945 - val_loss: 61197.5979 - val_mae: 61197.5977\n",
      "Epoch 162/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 61814.6378 - mae: 61814.6523 - val_loss: 61160.5493 - val_mae: 61160.5430\n",
      "Epoch 163/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 61748.7036 - mae: 61748.7188 - val_loss: 61126.6568 - val_mae: 61126.6562\n",
      "Epoch 164/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 61680.6790 - mae: 61680.6836 - val_loss: 61079.1156 - val_mae: 61079.1172\n",
      "Epoch 165/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 61611.5627 - mae: 61611.5664 - val_loss: 61032.7334 - val_mae: 61032.7305\n",
      "Epoch 166/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 61538.2718 - mae: 61538.2891 - val_loss: 61001.5334 - val_mae: 61001.5312\n",
      "Epoch 167/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 61468.6841 - mae: 61468.6875 - val_loss: 60986.4602 - val_mae: 60986.4531\n",
      "Epoch 168/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 61415.3523 - mae: 61415.3438 - val_loss: 60971.7344 - val_mae: 60971.7383\n",
      "Epoch 169/200\n",
      "1063/1063 [==============================] - 0s 107us/step - loss: 61362.5894 - mae: 61362.5938 - val_loss: 60953.8519 - val_mae: 60953.8477\n",
      "Epoch 170/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 61312.1138 - mae: 61312.1172 - val_loss: 60939.8123 - val_mae: 60939.8125\n",
      "Epoch 171/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 61263.4107 - mae: 61263.3984 - val_loss: 60923.1063 - val_mae: 60923.0977\n",
      "Epoch 172/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 61211.7122 - mae: 61211.7266 - val_loss: 60906.3680 - val_mae: 60906.3672\n",
      "Epoch 173/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 61159.1933 - mae: 61159.1992 - val_loss: 60889.4226 - val_mae: 60889.4219\n",
      "Epoch 174/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 61108.9423 - mae: 61108.9492 - val_loss: 60879.0734 - val_mae: 60879.0742\n",
      "Epoch 175/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 61069.4990 - mae: 61069.4922 - val_loss: 60870.0305 - val_mae: 60870.0234\n",
      "Epoch 176/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 61027.2737 - mae: 61027.2656 - val_loss: 60860.8101 - val_mae: 60860.8242\n",
      "Epoch 177/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60988.7516 - mae: 60988.7461 - val_loss: 60851.4917 - val_mae: 60851.5039\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 99us/step - loss: 60950.9471 - mae: 60950.9492 - val_loss: 60849.0824 - val_mae: 60849.0664\n",
      "Epoch 179/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60914.2424 - mae: 60914.2383 - val_loss: 60840.7359 - val_mae: 60840.7344\n",
      "Epoch 180/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60876.1496 - mae: 60876.1523 - val_loss: 60838.3055 - val_mae: 60838.3047\n",
      "Epoch 181/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60838.1028 - mae: 60838.0898 - val_loss: 60833.8443 - val_mae: 60833.8438\n",
      "Epoch 182/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 60801.0986 - mae: 60801.1055 - val_loss: 60834.2267 - val_mae: 60834.2188\n",
      "Epoch 183/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 60767.1007 - mae: 60767.1055 - val_loss: 60827.9390 - val_mae: 60827.9219\n",
      "Epoch 184/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 60732.5155 - mae: 60732.5234 - val_loss: 60817.3665 - val_mae: 60817.3711\n",
      "Epoch 185/200\n",
      "1063/1063 [==============================] - 0s 96us/step - loss: 60697.9083 - mae: 60697.9062 - val_loss: 60815.5046 - val_mae: 60815.5039\n",
      "Epoch 186/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60661.0764 - mae: 60661.0781 - val_loss: 60810.1124 - val_mae: 60810.1016\n",
      "Epoch 187/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 60626.8585 - mae: 60626.8477 - val_loss: 60806.8131 - val_mae: 60806.8086\n",
      "Epoch 188/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60601.2527 - mae: 60601.2539 - val_loss: 60803.6671 - val_mae: 60803.6797\n",
      "Epoch 189/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60574.4587 - mae: 60574.4531 - val_loss: 60800.2377 - val_mae: 60800.2305\n",
      "Epoch 190/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60547.0538 - mae: 60547.0391 - val_loss: 60798.3906 - val_mae: 60798.3906\n",
      "Epoch 191/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60517.5193 - mae: 60517.5273 - val_loss: 60790.3379 - val_mae: 60790.3320\n",
      "Epoch 192/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60489.2954 - mae: 60489.3047 - val_loss: 60786.2404 - val_mae: 60786.2422\n",
      "Epoch 193/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 60461.5151 - mae: 60461.5039 - val_loss: 60784.5099 - val_mae: 60784.5078\n",
      "Epoch 194/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60436.0205 - mae: 60436.0156 - val_loss: 60785.0776 - val_mae: 60785.0781\n",
      "Epoch 195/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 60408.5983 - mae: 60408.5977 - val_loss: 60786.2133 - val_mae: 60786.2109\n",
      "Epoch 196/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 60383.4067 - mae: 60383.3906 - val_loss: 60791.1205 - val_mae: 60791.1172\n",
      "Epoch 197/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60359.1069 - mae: 60359.1172 - val_loss: 60793.5327 - val_mae: 60793.5352\n",
      "Epoch 198/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 60333.2514 - mae: 60333.2617 - val_loss: 60797.0778 - val_mae: 60797.0664\n",
      "Epoch 199/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 60311.8547 - mae: 60311.8516 - val_loss: 60797.6192 - val_mae: 60797.6211\n",
      "Epoch 200/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 60288.6226 - mae: 60288.6172 - val_loss: 60800.7426 - val_mae: 60800.7461\n",
      "processing fold # 3\n",
      "Train on 1063 samples, validate on 354 samples\n",
      "Epoch 1/200\n",
      "1063/1063 [==============================] - 0s 166us/step - loss: 136728.0172 - mae: 136728.0156 - val_loss: 132475.3995 - val_mae: 132475.3750\n",
      "Epoch 2/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 136725.5291 - mae: 136725.5312 - val_loss: 132472.0314 - val_mae: 132472.0156\n",
      "Epoch 3/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 136720.8435 - mae: 136720.8438 - val_loss: 132465.8632 - val_mae: 132465.8594\n",
      "Epoch 4/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 136712.9371 - mae: 136712.9531 - val_loss: 132456.1264 - val_mae: 132456.1094\n",
      "Epoch 5/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 136701.0544 - mae: 136701.0469 - val_loss: 132441.8951 - val_mae: 132441.9219\n",
      "Epoch 6/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 136684.1143 - mae: 136684.1094 - val_loss: 132422.2867 - val_mae: 132422.2969\n",
      "Epoch 7/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 136661.6055 - mae: 136661.5312 - val_loss: 132396.3453 - val_mae: 132396.3438\n",
      "Epoch 8/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 136632.1436 - mae: 136632.0938 - val_loss: 132363.2219 - val_mae: 132363.2031\n",
      "Epoch 9/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 136595.3824 - mae: 136595.3750 - val_loss: 132322.3091 - val_mae: 132322.3125\n",
      "Epoch 10/200\n",
      "1063/1063 [==============================] - 0s 113us/step - loss: 136550.0549 - mae: 136550.0312 - val_loss: 132272.1112 - val_mae: 132272.0938\n",
      "Epoch 11/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 136494.7418 - mae: 136494.7500 - val_loss: 132211.7168 - val_mae: 132211.7344\n",
      "Epoch 12/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 136430.2159 - mae: 136430.2344 - val_loss: 132141.3167 - val_mae: 132141.2969\n",
      "Epoch 13/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 136354.3930 - mae: 136354.4062 - val_loss: 132059.2632 - val_mae: 132059.2812\n",
      "Epoch 14/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 136266.8312 - mae: 136266.8594 - val_loss: 131965.2406 - val_mae: 131965.2031\n",
      "Epoch 15/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 136166.8319 - mae: 136166.7812 - val_loss: 131856.7368 - val_mae: 131856.7188\n",
      "Epoch 16/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 136051.8789 - mae: 136051.8906 - val_loss: 131733.4688 - val_mae: 131733.4531\n",
      "Epoch 17/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 135922.6327 - mae: 135922.6562 - val_loss: 131595.3040 - val_mae: 131595.2969\n",
      "Epoch 18/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 135778.6714 - mae: 135778.6562 - val_loss: 131442.3589 - val_mae: 131442.3594\n",
      "Epoch 19/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 135617.0729 - mae: 135617.0625 - val_loss: 131271.3672 - val_mae: 131271.3750\n",
      "Epoch 20/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 135439.5091 - mae: 135439.5469 - val_loss: 131082.5962 - val_mae: 131082.5938\n",
      "Epoch 21/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 135242.9595 - mae: 135242.9844 - val_loss: 130875.3234 - val_mae: 130875.3359\n",
      "Epoch 22/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 135028.4887 - mae: 135028.5156 - val_loss: 130648.5480 - val_mae: 130648.5312\n",
      "Epoch 23/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 134793.0761 - mae: 134793.0781 - val_loss: 130400.9177 - val_mae: 130400.9141\n",
      "Epoch 24/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 134537.6470 - mae: 134537.6250 - val_loss: 130132.2769 - val_mae: 130132.2812\n",
      "Epoch 25/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 134261.5526 - mae: 134261.5312 - val_loss: 129842.7743 - val_mae: 129842.7656\n",
      "Epoch 26/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 133965.0440 - mae: 133965.0625 - val_loss: 129531.0501 - val_mae: 129531.0625\n",
      "Epoch 27/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 133641.9250 - mae: 133641.8906 - val_loss: 129193.3513 - val_mae: 129193.3594\n",
      "Epoch 28/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 133297.5585 - mae: 133297.5781 - val_loss: 128832.5161 - val_mae: 128832.5312\n",
      "Epoch 29/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 132928.2452 - mae: 132928.2500 - val_loss: 128445.8792 - val_mae: 128445.8672\n",
      "Epoch 30/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 101us/step - loss: 132533.8962 - mae: 132533.9375 - val_loss: 128031.6035 - val_mae: 128031.6016\n",
      "Epoch 31/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 132108.2974 - mae: 132108.2969 - val_loss: 127589.7565 - val_mae: 127589.7656\n",
      "Epoch 32/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 131655.9837 - mae: 131655.9844 - val_loss: 127114.0674 - val_mae: 127114.0781\n",
      "Epoch 33/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 131170.7316 - mae: 131170.7500 - val_loss: 126612.2523 - val_mae: 126612.2578\n",
      "Epoch 34/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 130659.2165 - mae: 130659.1875 - val_loss: 126078.9485 - val_mae: 126078.9609\n",
      "Epoch 35/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 130119.3145 - mae: 130119.2812 - val_loss: 125519.1925 - val_mae: 125519.1953\n",
      "Epoch 36/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 129549.4372 - mae: 129549.4297 - val_loss: 124923.3533 - val_mae: 124923.3438\n",
      "Epoch 37/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 128944.5211 - mae: 128944.5156 - val_loss: 124296.9715 - val_mae: 124296.9688\n",
      "Epoch 38/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 128306.4305 - mae: 128306.4375 - val_loss: 123636.3716 - val_mae: 123636.3750\n",
      "Epoch 39/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 127638.6400 - mae: 127638.6094 - val_loss: 122944.9290 - val_mae: 122944.9297\n",
      "Epoch 40/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 126940.0417 - mae: 126940.0703 - val_loss: 122219.5370 - val_mae: 122219.5469\n",
      "Epoch 41/200\n",
      "1063/1063 [==============================] - 0s 108us/step - loss: 126205.3552 - mae: 126205.3438 - val_loss: 121460.3616 - val_mae: 121460.3359\n",
      "Epoch 42/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 125434.4117 - mae: 125434.4531 - val_loss: 120677.2849 - val_mae: 120677.2891\n",
      "Epoch 43/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 124657.7263 - mae: 124657.7422 - val_loss: 119911.0848 - val_mae: 119911.0859\n",
      "Epoch 44/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 123877.0123 - mae: 123877.0000 - val_loss: 119146.0761 - val_mae: 119146.0703\n",
      "Epoch 45/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 123107.8732 - mae: 123107.8906 - val_loss: 118397.7126 - val_mae: 118397.7266\n",
      "Epoch 46/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 122352.4179 - mae: 122352.4219 - val_loss: 117651.7552 - val_mae: 117651.7500\n",
      "Epoch 47/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 121587.4220 - mae: 121587.4375 - val_loss: 116899.3140 - val_mae: 116899.3359\n",
      "Epoch 48/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 120850.1453 - mae: 120850.1641 - val_loss: 116150.0911 - val_mae: 116150.1016\n",
      "Epoch 49/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 120092.8139 - mae: 120092.8359 - val_loss: 115385.0451 - val_mae: 115385.0547\n",
      "Epoch 50/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 119326.8054 - mae: 119326.8047 - val_loss: 114607.8577 - val_mae: 114607.8672\n",
      "Epoch 51/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 118560.0231 - mae: 118560.0547 - val_loss: 113810.5130 - val_mae: 113810.5078\n",
      "Epoch 52/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 117745.6775 - mae: 117745.6719 - val_loss: 112967.1962 - val_mae: 112967.1875\n",
      "Epoch 53/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 116932.7599 - mae: 116932.7422 - val_loss: 112135.8743 - val_mae: 112135.8516\n",
      "Epoch 54/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 116105.6666 - mae: 116105.6953 - val_loss: 111296.9244 - val_mae: 111296.9297\n",
      "Epoch 55/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 115223.5401 - mae: 115223.5312 - val_loss: 110430.4198 - val_mae: 110430.4219\n",
      "Epoch 56/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 114352.0175 - mae: 114352.0391 - val_loss: 109556.8875 - val_mae: 109556.8906\n",
      "Epoch 57/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 113473.6495 - mae: 113473.6484 - val_loss: 108673.3956 - val_mae: 108673.4141\n",
      "Epoch 58/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 112586.5592 - mae: 112586.5625 - val_loss: 107761.6745 - val_mae: 107761.6953\n",
      "Epoch 59/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 111696.4275 - mae: 111696.4062 - val_loss: 106849.1231 - val_mae: 106849.1094\n",
      "Epoch 60/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 110781.5143 - mae: 110781.4844 - val_loss: 105924.9789 - val_mae: 105924.9922\n",
      "Epoch 61/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 109832.0254 - mae: 109832.0312 - val_loss: 104970.6368 - val_mae: 104970.6250\n",
      "Epoch 62/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 108871.9657 - mae: 108871.9688 - val_loss: 103999.8543 - val_mae: 103999.8516\n",
      "Epoch 63/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 107878.8835 - mae: 107878.9141 - val_loss: 102990.5779 - val_mae: 102990.5781\n",
      "Epoch 64/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 106880.8457 - mae: 106880.8359 - val_loss: 101957.8518 - val_mae: 101957.8516\n",
      "Epoch 65/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 105848.9309 - mae: 105848.9453 - val_loss: 100922.7384 - val_mae: 100922.7422\n",
      "Epoch 66/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 104829.3610 - mae: 104829.3516 - val_loss: 99853.8217 - val_mae: 99853.8281\n",
      "Epoch 67/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 103760.0862 - mae: 103760.1016 - val_loss: 98752.9629 - val_mae: 98752.9688\n",
      "Epoch 68/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 102673.4375 - mae: 102673.4297 - val_loss: 97645.3376 - val_mae: 97645.3438\n",
      "Epoch 69/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 101607.9084 - mae: 101607.9062 - val_loss: 96521.7279 - val_mae: 96521.7422\n",
      "Epoch 70/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 100494.9274 - mae: 100494.9297 - val_loss: 95398.2230 - val_mae: 95398.2266\n",
      "Epoch 71/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 99414.1239 - mae: 99414.1250 - val_loss: 94293.6064 - val_mae: 94293.6094\n",
      "Epoch 72/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 98345.3534 - mae: 98345.3672 - val_loss: 93189.5942 - val_mae: 93189.5859\n",
      "Epoch 73/200\n",
      "1063/1063 [==============================] - 0s 107us/step - loss: 97272.3949 - mae: 97272.4062 - val_loss: 92076.9942 - val_mae: 92076.9922\n",
      "Epoch 74/200\n",
      "1063/1063 [==============================] - 0s 127us/step - loss: 96220.4891 - mae: 96220.4688 - val_loss: 90988.4870 - val_mae: 90988.4922\n",
      "Epoch 75/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 95167.7073 - mae: 95167.7188 - val_loss: 89910.5580 - val_mae: 89910.5781\n",
      "Epoch 76/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 94134.2568 - mae: 94134.2578 - val_loss: 88855.1060 - val_mae: 88855.1094\n",
      "Epoch 77/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 93105.4560 - mae: 93105.4453 - val_loss: 87807.2742 - val_mae: 87807.2812\n",
      "Epoch 78/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 92096.5795 - mae: 92096.5781 - val_loss: 86777.4989 - val_mae: 86777.5000\n",
      "Epoch 79/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 91096.4948 - mae: 91096.5156 - val_loss: 85736.1367 - val_mae: 85736.1328\n",
      "Epoch 80/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 90138.2350 - mae: 90138.2422 - val_loss: 84720.6277 - val_mae: 84720.6328\n",
      "Epoch 81/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 89173.5217 - mae: 89173.5156 - val_loss: 83729.5115 - val_mae: 83729.5000\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 100us/step - loss: 88217.7868 - mae: 88217.7812 - val_loss: 82752.9907 - val_mae: 82753.0078\n",
      "Epoch 83/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 87335.3178 - mae: 87335.3281 - val_loss: 81834.0648 - val_mae: 81834.0703\n",
      "Epoch 84/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 86447.3432 - mae: 86447.3438 - val_loss: 80908.3232 - val_mae: 80908.3359\n",
      "Epoch 85/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 85593.3312 - mae: 85593.3359 - val_loss: 80006.2850 - val_mae: 80006.2891\n",
      "Epoch 86/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 84724.0921 - mae: 84724.0938 - val_loss: 79110.0791 - val_mae: 79110.0781\n",
      "Epoch 87/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 83892.0469 - mae: 83892.0391 - val_loss: 78224.0411 - val_mae: 78224.0391\n",
      "Epoch 88/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 83058.6320 - mae: 83058.6094 - val_loss: 77337.5900 - val_mae: 77337.5781\n",
      "Epoch 89/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 82262.7521 - mae: 82262.7578 - val_loss: 76484.7595 - val_mae: 76484.7578\n",
      "Epoch 90/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 81526.1071 - mae: 81526.1016 - val_loss: 75659.0023 - val_mae: 75659.0000\n",
      "Epoch 91/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 80830.5950 - mae: 80830.6016 - val_loss: 74864.4062 - val_mae: 74864.4141\n",
      "Epoch 92/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 80126.6995 - mae: 80126.7109 - val_loss: 74086.1070 - val_mae: 74086.1094\n",
      "Epoch 93/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 79425.1726 - mae: 79425.1641 - val_loss: 73330.7711 - val_mae: 73330.7891\n",
      "Epoch 94/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 78755.5556 - mae: 78755.5781 - val_loss: 72578.2542 - val_mae: 72578.2422\n",
      "Epoch 95/200\n",
      "1063/1063 [==============================] - 0s 107us/step - loss: 78116.7458 - mae: 78116.7656 - val_loss: 71839.6635 - val_mae: 71839.6719\n",
      "Epoch 96/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 77517.5982 - mae: 77517.6094 - val_loss: 71149.6530 - val_mae: 71149.6562\n",
      "Epoch 97/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 76948.6204 - mae: 76948.6172 - val_loss: 70497.1256 - val_mae: 70497.1250\n",
      "Epoch 98/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 76399.8269 - mae: 76399.7969 - val_loss: 69862.6965 - val_mae: 69862.6953\n",
      "Epoch 99/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 75878.4883 - mae: 75878.4766 - val_loss: 69271.2085 - val_mae: 69271.2109\n",
      "Epoch 100/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 75396.1437 - mae: 75396.1562 - val_loss: 68733.8851 - val_mae: 68733.8828\n",
      "Epoch 101/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 74953.8808 - mae: 74953.8906 - val_loss: 68220.8785 - val_mae: 68220.8906\n",
      "Epoch 102/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 74508.3700 - mae: 74508.3672 - val_loss: 67709.2532 - val_mae: 67709.2500\n",
      "Epoch 103/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 74118.5252 - mae: 74118.5156 - val_loss: 67297.8983 - val_mae: 67297.8906\n",
      "Epoch 104/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 73755.5448 - mae: 73755.5391 - val_loss: 66903.3746 - val_mae: 66903.3828\n",
      "Epoch 105/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 73392.4456 - mae: 73392.4297 - val_loss: 66503.3381 - val_mae: 66503.3359\n",
      "Epoch 106/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 73045.7797 - mae: 73045.7969 - val_loss: 66152.1331 - val_mae: 66152.1328\n",
      "Epoch 107/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 72731.6927 - mae: 72731.7031 - val_loss: 65824.0975 - val_mae: 65824.0938\n",
      "Epoch 108/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 72414.7914 - mae: 72414.7812 - val_loss: 65489.6348 - val_mae: 65489.6289\n",
      "Epoch 109/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 72100.5954 - mae: 72100.5859 - val_loss: 65195.0656 - val_mae: 65195.0664\n",
      "Epoch 110/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 71799.9940 - mae: 71800.0312 - val_loss: 64895.8615 - val_mae: 64895.8516\n",
      "Epoch 111/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 71525.6222 - mae: 71525.6406 - val_loss: 64623.6481 - val_mae: 64623.6484\n",
      "Epoch 112/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 71257.8815 - mae: 71257.8672 - val_loss: 64350.4502 - val_mae: 64350.4531\n",
      "Epoch 113/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 71000.2437 - mae: 71000.2500 - val_loss: 64074.3178 - val_mae: 64074.3125\n",
      "Epoch 114/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 70741.3232 - mae: 70741.3359 - val_loss: 63807.1088 - val_mae: 63807.1133\n",
      "Epoch 115/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 70503.4291 - mae: 70503.4531 - val_loss: 63571.9855 - val_mae: 63571.9727\n",
      "Epoch 116/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 70271.2533 - mae: 70271.2422 - val_loss: 63337.7263 - val_mae: 63337.7305\n",
      "Epoch 117/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 70038.8197 - mae: 70038.8359 - val_loss: 63114.8637 - val_mae: 63114.8633\n",
      "Epoch 118/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 69818.3311 - mae: 69818.3359 - val_loss: 62913.3717 - val_mae: 62913.3672\n",
      "Epoch 119/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 69608.7494 - mae: 69608.7656 - val_loss: 62723.6448 - val_mae: 62723.6367\n",
      "Epoch 120/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 69403.0656 - mae: 69403.0703 - val_loss: 62537.4263 - val_mae: 62537.4141\n",
      "Epoch 121/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 69196.9103 - mae: 69196.9375 - val_loss: 62350.8401 - val_mae: 62350.8477\n",
      "Epoch 122/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 69005.8833 - mae: 69005.8750 - val_loss: 62168.9858 - val_mae: 62168.9961\n",
      "Epoch 123/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 68802.0673 - mae: 68802.0625 - val_loss: 61977.3860 - val_mae: 61977.3828\n",
      "Epoch 124/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 68616.9761 - mae: 68616.9922 - val_loss: 61808.9232 - val_mae: 61808.9141\n",
      "Epoch 125/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 68440.1468 - mae: 68440.1406 - val_loss: 61633.4074 - val_mae: 61633.4141\n",
      "Epoch 126/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 68272.0983 - mae: 68272.1016 - val_loss: 61476.2550 - val_mae: 61476.2539\n",
      "Epoch 127/200\n",
      "1063/1063 [==============================] - 0s 107us/step - loss: 68125.1551 - mae: 68125.1641 - val_loss: 61320.9526 - val_mae: 61320.9531\n",
      "Epoch 128/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 67977.3216 - mae: 67977.3359 - val_loss: 61182.6416 - val_mae: 61182.6562\n",
      "Epoch 129/200\n",
      "1063/1063 [==============================] - 0s 108us/step - loss: 67842.7502 - mae: 67842.7578 - val_loss: 61050.0706 - val_mae: 61050.0781\n",
      "Epoch 130/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 67702.7010 - mae: 67702.6953 - val_loss: 60911.2226 - val_mae: 60911.2266\n",
      "Epoch 131/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 67561.2876 - mae: 67561.2969 - val_loss: 60778.1095 - val_mae: 60778.1055\n",
      "Epoch 132/200\n",
      "1063/1063 [==============================] - 0s 128us/step - loss: 67419.5962 - mae: 67419.5938 - val_loss: 60653.5406 - val_mae: 60653.5469\n",
      "Epoch 133/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 67284.1624 - mae: 67284.1641 - val_loss: 60532.8032 - val_mae: 60532.8086\n",
      "Epoch 134/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 67155.1793 - mae: 67155.1641 - val_loss: 60427.1481 - val_mae: 60427.1562\n",
      "Epoch 135/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 101us/step - loss: 67031.7278 - mae: 67031.7266 - val_loss: 60319.2105 - val_mae: 60319.2031\n",
      "Epoch 136/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 66913.9034 - mae: 66913.9062 - val_loss: 60212.9071 - val_mae: 60212.9102\n",
      "Epoch 137/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 66789.4841 - mae: 66789.5000 - val_loss: 60116.4946 - val_mae: 60116.4805\n",
      "Epoch 138/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 66672.6095 - mae: 66672.6484 - val_loss: 60011.6645 - val_mae: 60011.6602\n",
      "Epoch 139/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 66554.8411 - mae: 66554.8281 - val_loss: 59911.7812 - val_mae: 59911.7812\n",
      "Epoch 140/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 66451.3830 - mae: 66451.3828 - val_loss: 59820.0085 - val_mae: 59820.0156\n",
      "Epoch 141/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 66359.3042 - mae: 66359.3047 - val_loss: 59728.4665 - val_mae: 59728.4648\n",
      "Epoch 142/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 66266.5717 - mae: 66266.5781 - val_loss: 59634.5731 - val_mae: 59634.5820\n",
      "Epoch 143/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 66179.4539 - mae: 66179.4531 - val_loss: 59552.6411 - val_mae: 59552.6328\n",
      "Epoch 144/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 66100.1460 - mae: 66100.1406 - val_loss: 59463.5833 - val_mae: 59463.5781\n",
      "Epoch 145/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 66014.4791 - mae: 66014.4766 - val_loss: 59381.7802 - val_mae: 59381.7812\n",
      "Epoch 146/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 65937.9974 - mae: 65937.9922 - val_loss: 59307.1376 - val_mae: 59307.1289\n",
      "Epoch 147/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 65862.5152 - mae: 65862.5000 - val_loss: 59230.8174 - val_mae: 59230.8086\n",
      "Epoch 148/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 65787.3683 - mae: 65787.3750 - val_loss: 59151.7846 - val_mae: 59151.7852\n",
      "Epoch 149/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 65718.6862 - mae: 65718.6875 - val_loss: 59082.0669 - val_mae: 59082.0547\n",
      "Epoch 150/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 65649.5554 - mae: 65649.5547 - val_loss: 59011.9417 - val_mae: 59011.9336\n",
      "Epoch 151/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 65581.7229 - mae: 65581.7109 - val_loss: 58940.6564 - val_mae: 58940.6484\n",
      "Epoch 152/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 65512.2098 - mae: 65512.1875 - val_loss: 58868.5679 - val_mae: 58868.5586\n",
      "Epoch 153/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 65448.1496 - mae: 65448.1562 - val_loss: 58804.3200 - val_mae: 58804.3281\n",
      "Epoch 154/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 65380.5296 - mae: 65380.5391 - val_loss: 58736.7540 - val_mae: 58736.7578\n",
      "Epoch 155/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 65316.3647 - mae: 65316.3633 - val_loss: 58668.7623 - val_mae: 58668.7617\n",
      "Epoch 156/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 65246.4265 - mae: 65246.4336 - val_loss: 58602.3908 - val_mae: 58602.3828\n",
      "Epoch 157/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 65174.3532 - mae: 65174.3359 - val_loss: 58527.6582 - val_mae: 58527.6602\n",
      "Epoch 158/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 65110.4460 - mae: 65110.4414 - val_loss: 58459.8079 - val_mae: 58459.8125\n",
      "Epoch 159/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 65042.5179 - mae: 65042.5195 - val_loss: 58391.7869 - val_mae: 58391.8008\n",
      "Epoch 160/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 64976.6612 - mae: 64976.6562 - val_loss: 58321.3348 - val_mae: 58321.3203\n",
      "Epoch 161/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 64910.1577 - mae: 64910.1562 - val_loss: 58252.9725 - val_mae: 58252.9648\n",
      "Epoch 162/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 64839.4354 - mae: 64839.4297 - val_loss: 58180.0023 - val_mae: 58180.0039\n",
      "Epoch 163/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 64777.9664 - mae: 64777.9727 - val_loss: 58117.5985 - val_mae: 58117.5938\n",
      "Epoch 164/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 64722.1769 - mae: 64722.1836 - val_loss: 58055.8084 - val_mae: 58055.8008\n",
      "Epoch 165/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 64664.1242 - mae: 64664.1289 - val_loss: 57993.8422 - val_mae: 57993.8516\n",
      "Epoch 166/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 64608.4914 - mae: 64608.5039 - val_loss: 57929.8948 - val_mae: 57929.9023\n",
      "Epoch 167/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 64554.0434 - mae: 64554.0312 - val_loss: 57879.8143 - val_mae: 57879.8125\n",
      "Epoch 168/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 64509.0286 - mae: 64509.0352 - val_loss: 57820.8648 - val_mae: 57820.8594\n",
      "Epoch 169/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 64461.2294 - mae: 64461.2305 - val_loss: 57768.6193 - val_mae: 57768.6211\n",
      "Epoch 170/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 64416.6975 - mae: 64416.6914 - val_loss: 57713.3872 - val_mae: 57713.3906\n",
      "Epoch 171/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 64368.9709 - mae: 64368.9727 - val_loss: 57667.1959 - val_mae: 57667.1992\n",
      "Epoch 172/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 64322.7366 - mae: 64322.7383 - val_loss: 57621.3701 - val_mae: 57621.3633\n",
      "Epoch 173/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 64280.4806 - mae: 64280.4883 - val_loss: 57586.9776 - val_mae: 57586.9727\n",
      "Epoch 174/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 64239.3687 - mae: 64239.3672 - val_loss: 57555.0247 - val_mae: 57555.0273\n",
      "Epoch 175/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 64198.3510 - mae: 64198.3750 - val_loss: 57520.3284 - val_mae: 57520.3398\n",
      "Epoch 176/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 64159.1779 - mae: 64159.1875 - val_loss: 57485.6472 - val_mae: 57485.6445\n",
      "Epoch 177/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 64116.3686 - mae: 64116.3867 - val_loss: 57449.1763 - val_mae: 57449.1797\n",
      "Epoch 178/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 64076.6626 - mae: 64076.6602 - val_loss: 57416.0467 - val_mae: 57416.0469\n",
      "Epoch 179/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 64034.7568 - mae: 64034.7695 - val_loss: 57382.7144 - val_mae: 57382.7227\n",
      "Epoch 180/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 63996.8320 - mae: 63996.8477 - val_loss: 57355.9641 - val_mae: 57355.9609\n",
      "Epoch 181/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 63958.8854 - mae: 63958.8945 - val_loss: 57325.3189 - val_mae: 57325.3164\n",
      "Epoch 182/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 63918.7230 - mae: 63918.7266 - val_loss: 57294.8819 - val_mae: 57294.8867\n",
      "Epoch 183/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 63882.8021 - mae: 63882.8008 - val_loss: 57266.8181 - val_mae: 57266.8242\n",
      "Epoch 184/200\n",
      "1063/1063 [==============================] - 0s 97us/step - loss: 63851.6883 - mae: 63851.6875 - val_loss: 57236.2166 - val_mae: 57236.2148\n",
      "Epoch 185/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 63819.0805 - mae: 63819.0703 - val_loss: 57212.2997 - val_mae: 57212.3008\n",
      "Epoch 186/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 63785.9538 - mae: 63785.9570 - val_loss: 57183.6463 - val_mae: 57183.6484\n",
      "Epoch 187/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 63750.7742 - mae: 63750.7891 - val_loss: 57157.8845 - val_mae: 57157.8945\n",
      "Epoch 188/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 98us/step - loss: 63720.7301 - mae: 63720.7305 - val_loss: 57132.8283 - val_mae: 57132.8320\n",
      "Epoch 189/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 63689.1800 - mae: 63689.1602 - val_loss: 57106.8498 - val_mae: 57106.8438\n",
      "Epoch 190/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 63657.5213 - mae: 63657.5352 - val_loss: 57079.8020 - val_mae: 57079.8086\n",
      "Epoch 191/200\n",
      "1063/1063 [==============================] - 0s 106us/step - loss: 63625.5948 - mae: 63625.5898 - val_loss: 57052.8649 - val_mae: 57052.8594\n",
      "Epoch 192/200\n",
      "1063/1063 [==============================] - 0s 104us/step - loss: 63592.7774 - mae: 63592.7617 - val_loss: 57029.3478 - val_mae: 57029.3398\n",
      "Epoch 193/200\n",
      "1063/1063 [==============================] - 0s 105us/step - loss: 63562.5011 - mae: 63562.4922 - val_loss: 57002.1663 - val_mae: 57002.1680\n",
      "Epoch 194/200\n",
      "1063/1063 [==============================] - 0s 102us/step - loss: 63530.6552 - mae: 63530.6641 - val_loss: 56975.5572 - val_mae: 56975.5586\n",
      "Epoch 195/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 63500.3623 - mae: 63500.3633 - val_loss: 56950.3416 - val_mae: 56950.3398\n",
      "Epoch 196/200\n",
      "1063/1063 [==============================] - 0s 101us/step - loss: 63469.2497 - mae: 63469.2305 - val_loss: 56925.7333 - val_mae: 56925.7344\n",
      "Epoch 197/200\n",
      "1063/1063 [==============================] - 0s 99us/step - loss: 63438.5720 - mae: 63438.5703 - val_loss: 56898.4050 - val_mae: 56898.4023\n",
      "Epoch 198/200\n",
      "1063/1063 [==============================] - 0s 98us/step - loss: 63404.3379 - mae: 63404.3594 - val_loss: 56872.7420 - val_mae: 56872.7344\n",
      "Epoch 199/200\n",
      "1063/1063 [==============================] - 0s 100us/step - loss: 63371.9187 - mae: 63371.9141 - val_loss: 56849.3674 - val_mae: 56849.3711\n",
      "Epoch 200/200\n",
      "1063/1063 [==============================] - 0s 103us/step - loss: 63341.8631 - mae: 63341.8633 - val_loss: 56822.7252 - val_mae: 56822.7188\n",
      "It cost 89.315635 sec\n",
      "89.31563472747803\n"
     ]
    }
   ],
   "source": [
    "tStart = time.time()\n",
    "all_mae_histories=[]\n",
    "for i in range(k):\n",
    "    print('processing fold #',i)\n",
    "    val_data = train_data[i * num_val_samples: (i+1) * num_val_samples ]\n",
    "    val_targets = train_targets[i * num_val_samples:(i+1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate([train_data[:i * num_val_samples ],\n",
    "                                        train_data[(i+1)*num_val_samples:]],axis=0)\n",
    "    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples ],\n",
    "                                        train_targets[(i+1)*num_val_samples:]],axis=0)\n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, partial_train_targets,validation_data=(val_data, val_targets),epochs = num_epochs,batch_size=8)\n",
    "    mae_history = history.history['val_mae']\n",
    "    all_mae_histories.append(mae_history)\n",
    "tEnd = time.time()\n",
    "print (\"It cost %f sec\" % (tEnd - tStart))#會自動做近位\n",
    "print (tEnd - tStart)#原型長這樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [132475.39954096044,\n",
       "  132472.03142655367,\n",
       "  132465.86317090396,\n",
       "  132456.12641242938,\n",
       "  132441.89512711865,\n",
       "  132422.28672316385,\n",
       "  132396.34533898305,\n",
       "  132363.2219279661,\n",
       "  132322.30914548022,\n",
       "  132272.11122881356,\n",
       "  132211.7168079096,\n",
       "  132141.31673728814,\n",
       "  132059.26324152542,\n",
       "  131965.24064265538,\n",
       "  131856.73675847458,\n",
       "  131733.46875,\n",
       "  131595.30402542374,\n",
       "  131442.35893361582,\n",
       "  131271.3672316384,\n",
       "  131082.59622175142,\n",
       "  130875.32344632769,\n",
       "  130648.54802259887,\n",
       "  130400.91772598871,\n",
       "  130132.27692443503,\n",
       "  129842.77427612995,\n",
       "  129531.05014124294,\n",
       "  129193.35125353107,\n",
       "  128832.51606638418,\n",
       "  128445.87923728813,\n",
       "  128031.60346045198,\n",
       "  127589.75653248587,\n",
       "  127114.06744350282,\n",
       "  126612.25229519774,\n",
       "  126078.94853460453,\n",
       "  125519.19253177966,\n",
       "  124923.35328389831,\n",
       "  124296.97148658193,\n",
       "  123636.37164548022,\n",
       "  122944.92902542373,\n",
       "  122219.53698799435,\n",
       "  121460.36158192091,\n",
       "  120677.28486935029,\n",
       "  119911.08483403955,\n",
       "  119146.07609463276,\n",
       "  118397.71257062147,\n",
       "  117651.75520833333,\n",
       "  116899.31400070622,\n",
       "  116150.09110169491,\n",
       "  115385.04510946327,\n",
       "  114607.85769774011,\n",
       "  113810.51297669491,\n",
       "  112967.19623940678,\n",
       "  112135.87429378531,\n",
       "  111296.92443502825,\n",
       "  110430.41975635593,\n",
       "  109556.88753531073,\n",
       "  108673.39556850282,\n",
       "  107761.67452330509,\n",
       "  106849.1230579096,\n",
       "  105924.97890183616,\n",
       "  104970.63682909604,\n",
       "  103999.85425494351,\n",
       "  102990.57794844633,\n",
       "  101957.85178319209,\n",
       "  100922.73843573446,\n",
       "  99853.82168079096,\n",
       "  98752.96292372882,\n",
       "  97645.33757062147,\n",
       "  96521.72793079096,\n",
       "  95398.22298728813,\n",
       "  94293.60637358757,\n",
       "  93189.59419138418,\n",
       "  92076.99417372882,\n",
       "  90988.48702330509,\n",
       "  89910.55799788136,\n",
       "  88855.10602048022,\n",
       "  87807.27418785311,\n",
       "  86777.49885240113,\n",
       "  85736.1367408192,\n",
       "  84720.62769244351,\n",
       "  83729.51147598871,\n",
       "  82752.9907309322,\n",
       "  81834.06479519774,\n",
       "  80908.3232256356,\n",
       "  80006.28495762713,\n",
       "  79110.07905190678,\n",
       "  78224.04113700565,\n",
       "  77337.59004237287,\n",
       "  76484.75953389831,\n",
       "  75659.00225105933,\n",
       "  74864.40616172316,\n",
       "  74086.10699152542,\n",
       "  73330.77109816384,\n",
       "  72578.25423728813,\n",
       "  71839.66348870056,\n",
       "  71149.65298375706,\n",
       "  70497.12561793785,\n",
       "  69862.69650423729,\n",
       "  69271.208509887,\n",
       "  68733.88506355933,\n",
       "  68220.87848693503,\n",
       "  67709.25322210453,\n",
       "  67297.89826094633,\n",
       "  66903.37464689265,\n",
       "  66503.33814442091,\n",
       "  66152.13312146893,\n",
       "  65824.09745762713,\n",
       "  65489.63484286723,\n",
       "  65195.06563382768,\n",
       "  64895.861493644064,\n",
       "  64623.648084392655,\n",
       "  64350.45016772599,\n",
       "  64074.31779661017,\n",
       "  63807.108801200564,\n",
       "  63571.985478460454,\n",
       "  63337.72634180791,\n",
       "  63114.863656426554,\n",
       "  62913.37173375706,\n",
       "  62723.64481814972,\n",
       "  62537.42628884181,\n",
       "  62350.8400865113,\n",
       "  62168.9858315678,\n",
       "  61977.38603460452,\n",
       "  61808.92315501413,\n",
       "  61633.40744173729,\n",
       "  61476.25503177966,\n",
       "  61320.952551200564,\n",
       "  61182.64155190678,\n",
       "  61050.07057733051,\n",
       "  60911.22259004237,\n",
       "  60778.109463276836,\n",
       "  60653.540607344636,\n",
       "  60532.80318679378,\n",
       "  60427.148084392655,\n",
       "  60319.2104519774,\n",
       "  60212.907088629945,\n",
       "  60116.49461511299,\n",
       "  60011.6645480226,\n",
       "  59911.78120586158,\n",
       "  59820.00851871469,\n",
       "  59728.4665430791,\n",
       "  59634.57309322034,\n",
       "  59552.64106638418,\n",
       "  59463.58328919492,\n",
       "  59381.78023481638,\n",
       "  59307.13762358757,\n",
       "  59230.81735522599,\n",
       "  59151.784560381355,\n",
       "  59082.06691384181,\n",
       "  59011.941737288136,\n",
       "  58940.656382415254,\n",
       "  58868.56792902543,\n",
       "  58804.319959392655,\n",
       "  58736.754016596045,\n",
       "  58668.76227048023,\n",
       "  58602.390757415254,\n",
       "  58527.6581920904,\n",
       "  58459.8078654661,\n",
       "  58391.786943855936,\n",
       "  58321.334834039546,\n",
       "  58252.972545903955,\n",
       "  58180.00229519774,\n",
       "  58117.598516949154,\n",
       "  58055.80843926554,\n",
       "  57993.842205155364,\n",
       "  57929.89481814972,\n",
       "  57879.81426553673,\n",
       "  57820.864848163845,\n",
       "  57768.619262005646,\n",
       "  57713.38722634181,\n",
       "  57667.195930437854,\n",
       "  57621.37005649717,\n",
       "  57586.97757768362,\n",
       "  57555.02467337571,\n",
       "  57520.32843396893,\n",
       "  57485.64724576271,\n",
       "  57449.17628884181,\n",
       "  57416.046742584746,\n",
       "  57382.71438029661,\n",
       "  57355.9641154661,\n",
       "  57325.31890007062,\n",
       "  57294.88188559322,\n",
       "  57266.8181055791,\n",
       "  57236.216631355936,\n",
       "  57212.299699858755,\n",
       "  57183.646318855936,\n",
       "  57157.88453389831,\n",
       "  57132.828257415254,\n",
       "  57106.84984110169,\n",
       "  57079.80203919492,\n",
       "  57052.86493644068,\n",
       "  57029.347766596045,\n",
       "  57002.1662694209,\n",
       "  56975.557247528246,\n",
       "  56950.341631355936,\n",
       "  56925.733271539546,\n",
       "  56898.40501412429,\n",
       "  56872.74201094633,\n",
       "  56849.36736405367,\n",
       "  56822.72515007062],\n",
       " 'val_mae': [132475.375,\n",
       "  132472.015625,\n",
       "  132465.859375,\n",
       "  132456.109375,\n",
       "  132441.921875,\n",
       "  132422.296875,\n",
       "  132396.34375,\n",
       "  132363.203125,\n",
       "  132322.3125,\n",
       "  132272.09375,\n",
       "  132211.734375,\n",
       "  132141.296875,\n",
       "  132059.28125,\n",
       "  131965.203125,\n",
       "  131856.71875,\n",
       "  131733.453125,\n",
       "  131595.296875,\n",
       "  131442.359375,\n",
       "  131271.375,\n",
       "  131082.59375,\n",
       "  130875.3359375,\n",
       "  130648.53125,\n",
       "  130400.9140625,\n",
       "  130132.28125,\n",
       "  129842.765625,\n",
       "  129531.0625,\n",
       "  129193.359375,\n",
       "  128832.53125,\n",
       "  128445.8671875,\n",
       "  128031.6015625,\n",
       "  127589.765625,\n",
       "  127114.078125,\n",
       "  126612.2578125,\n",
       "  126078.9609375,\n",
       "  125519.1953125,\n",
       "  124923.34375,\n",
       "  124296.96875,\n",
       "  123636.375,\n",
       "  122944.9296875,\n",
       "  122219.546875,\n",
       "  121460.3359375,\n",
       "  120677.2890625,\n",
       "  119911.0859375,\n",
       "  119146.0703125,\n",
       "  118397.7265625,\n",
       "  117651.75,\n",
       "  116899.3359375,\n",
       "  116150.1015625,\n",
       "  115385.0546875,\n",
       "  114607.8671875,\n",
       "  113810.5078125,\n",
       "  112967.1875,\n",
       "  112135.8515625,\n",
       "  111296.9296875,\n",
       "  110430.421875,\n",
       "  109556.890625,\n",
       "  108673.4140625,\n",
       "  107761.6953125,\n",
       "  106849.109375,\n",
       "  105924.9921875,\n",
       "  104970.625,\n",
       "  103999.8515625,\n",
       "  102990.578125,\n",
       "  101957.8515625,\n",
       "  100922.7421875,\n",
       "  99853.828125,\n",
       "  98752.96875,\n",
       "  97645.34375,\n",
       "  96521.7421875,\n",
       "  95398.2265625,\n",
       "  94293.609375,\n",
       "  93189.5859375,\n",
       "  92076.9921875,\n",
       "  90988.4921875,\n",
       "  89910.578125,\n",
       "  88855.109375,\n",
       "  87807.28125,\n",
       "  86777.5,\n",
       "  85736.1328125,\n",
       "  84720.6328125,\n",
       "  83729.5,\n",
       "  82753.0078125,\n",
       "  81834.0703125,\n",
       "  80908.3359375,\n",
       "  80006.2890625,\n",
       "  79110.078125,\n",
       "  78224.0390625,\n",
       "  77337.578125,\n",
       "  76484.7578125,\n",
       "  75659.0,\n",
       "  74864.4140625,\n",
       "  74086.109375,\n",
       "  73330.7890625,\n",
       "  72578.2421875,\n",
       "  71839.671875,\n",
       "  71149.65625,\n",
       "  70497.125,\n",
       "  69862.6953125,\n",
       "  69271.2109375,\n",
       "  68733.8828125,\n",
       "  68220.890625,\n",
       "  67709.25,\n",
       "  67297.890625,\n",
       "  66903.3828125,\n",
       "  66503.3359375,\n",
       "  66152.1328125,\n",
       "  65824.09375,\n",
       "  65489.62890625,\n",
       "  65195.06640625,\n",
       "  64895.8515625,\n",
       "  64623.6484375,\n",
       "  64350.453125,\n",
       "  64074.3125,\n",
       "  63807.11328125,\n",
       "  63571.97265625,\n",
       "  63337.73046875,\n",
       "  63114.86328125,\n",
       "  62913.3671875,\n",
       "  62723.63671875,\n",
       "  62537.4140625,\n",
       "  62350.84765625,\n",
       "  62168.99609375,\n",
       "  61977.3828125,\n",
       "  61808.9140625,\n",
       "  61633.4140625,\n",
       "  61476.25390625,\n",
       "  61320.953125,\n",
       "  61182.65625,\n",
       "  61050.078125,\n",
       "  60911.2265625,\n",
       "  60778.10546875,\n",
       "  60653.546875,\n",
       "  60532.80859375,\n",
       "  60427.15625,\n",
       "  60319.203125,\n",
       "  60212.91015625,\n",
       "  60116.48046875,\n",
       "  60011.66015625,\n",
       "  59911.78125,\n",
       "  59820.015625,\n",
       "  59728.46484375,\n",
       "  59634.58203125,\n",
       "  59552.6328125,\n",
       "  59463.578125,\n",
       "  59381.78125,\n",
       "  59307.12890625,\n",
       "  59230.80859375,\n",
       "  59151.78515625,\n",
       "  59082.0546875,\n",
       "  59011.93359375,\n",
       "  58940.6484375,\n",
       "  58868.55859375,\n",
       "  58804.328125,\n",
       "  58736.7578125,\n",
       "  58668.76171875,\n",
       "  58602.3828125,\n",
       "  58527.66015625,\n",
       "  58459.8125,\n",
       "  58391.80078125,\n",
       "  58321.3203125,\n",
       "  58252.96484375,\n",
       "  58180.00390625,\n",
       "  58117.59375,\n",
       "  58055.80078125,\n",
       "  57993.8515625,\n",
       "  57929.90234375,\n",
       "  57879.8125,\n",
       "  57820.859375,\n",
       "  57768.62109375,\n",
       "  57713.390625,\n",
       "  57667.19921875,\n",
       "  57621.36328125,\n",
       "  57586.97265625,\n",
       "  57555.02734375,\n",
       "  57520.33984375,\n",
       "  57485.64453125,\n",
       "  57449.1796875,\n",
       "  57416.046875,\n",
       "  57382.72265625,\n",
       "  57355.9609375,\n",
       "  57325.31640625,\n",
       "  57294.88671875,\n",
       "  57266.82421875,\n",
       "  57236.21484375,\n",
       "  57212.30078125,\n",
       "  57183.6484375,\n",
       "  57157.89453125,\n",
       "  57132.83203125,\n",
       "  57106.84375,\n",
       "  57079.80859375,\n",
       "  57052.859375,\n",
       "  57029.33984375,\n",
       "  57002.16796875,\n",
       "  56975.55859375,\n",
       "  56950.33984375,\n",
       "  56925.734375,\n",
       "  56898.40234375,\n",
       "  56872.734375,\n",
       "  56849.37109375,\n",
       "  56822.71875],\n",
       " 'loss': [136728.01719778928,\n",
       "  136725.52908925212,\n",
       "  136720.84346337017,\n",
       "  136712.93706638052,\n",
       "  136701.0543567733,\n",
       "  136684.11434325023,\n",
       "  136661.6054797742,\n",
       "  136632.1436235889,\n",
       "  136595.38235683207,\n",
       "  136550.0548565381,\n",
       "  136494.7418126764,\n",
       "  136430.21586900283,\n",
       "  136354.3930282808,\n",
       "  136266.83115298682,\n",
       "  136166.83194673096,\n",
       "  136051.8788511289,\n",
       "  135922.6327316557,\n",
       "  135778.6713678857,\n",
       "  135617.0729289158,\n",
       "  135439.5090839605,\n",
       "  135242.95947495295,\n",
       "  135028.4886560736,\n",
       "  134793.07607449437,\n",
       "  134537.64698965193,\n",
       "  134261.55258554796,\n",
       "  133965.0439572554,\n",
       "  133641.92501322908,\n",
       "  133297.55853127938,\n",
       "  132928.24516404045,\n",
       "  132533.89621060676,\n",
       "  132108.29736006586,\n",
       "  131655.9836988476,\n",
       "  131170.73156014815,\n",
       "  130659.21649370884,\n",
       "  130119.31448436031,\n",
       "  129549.43721337018,\n",
       "  128944.52107831609,\n",
       "  128306.43045919568,\n",
       "  127638.64000029398,\n",
       "  126940.04170096426,\n",
       "  126205.35521519285,\n",
       "  125434.41167391816,\n",
       "  124657.7263126176,\n",
       "  123877.0122883349,\n",
       "  123107.8732287747,\n",
       "  122352.41787688147,\n",
       "  121587.42199259173,\n",
       "  120850.14525517404,\n",
       "  120092.81394784807,\n",
       "  119326.80538570085,\n",
       "  118560.02311412277,\n",
       "  117745.6774900047,\n",
       "  116932.75993650047,\n",
       "  116105.6665539746,\n",
       "  115223.54005468015,\n",
       "  114352.01754688969,\n",
       "  113473.6495325729,\n",
       "  112586.5592074318,\n",
       "  111696.42754145108,\n",
       "  110781.51432414158,\n",
       "  109832.02536306444,\n",
       "  108871.96567056679,\n",
       "  107878.88346660395,\n",
       "  106880.8456976129,\n",
       "  105848.93092956256,\n",
       "  104829.36095146401,\n",
       "  103760.0861873824,\n",
       "  102673.4374559031,\n",
       "  101607.90837400047,\n",
       "  100494.92743855833,\n",
       "  99414.12386817968,\n",
       "  98345.35336312324,\n",
       "  97272.39490974836,\n",
       "  96220.48910806679,\n",
       "  95167.70732706667,\n",
       "  94134.25683501881,\n",
       "  93105.45600599718,\n",
       "  92096.57953610066,\n",
       "  91096.49484433795,\n",
       "  90138.23498500706,\n",
       "  89173.52168832315,\n",
       "  88217.78676211195,\n",
       "  87335.31778427798,\n",
       "  86447.34315101716,\n",
       "  85593.33117871002,\n",
       "  84724.0921037159,\n",
       "  83892.04690439792,\n",
       "  83058.63204080432,\n",
       "  82262.75206520461,\n",
       "  81526.10708196143,\n",
       "  80830.59500676152,\n",
       "  80126.69952375353,\n",
       "  79425.1725952493,\n",
       "  78755.55558413688,\n",
       "  78116.74582549388,\n",
       "  77517.59817806326,\n",
       "  76948.62040657338,\n",
       "  76399.82687558795,\n",
       "  75878.48833453374,\n",
       "  75396.1436676858,\n",
       "  74953.88077669332,\n",
       "  74508.37002072555,\n",
       "  74118.52524547272,\n",
       "  73755.54478774694,\n",
       "  73392.4455623824,\n",
       "  73045.77967353599,\n",
       "  72731.6926960842,\n",
       "  72414.79142535865,\n",
       "  72100.59540363359,\n",
       "  71799.9940211959,\n",
       "  71525.62221454609,\n",
       "  71257.88154838899,\n",
       "  71000.24372354186,\n",
       "  70741.32323759407,\n",
       "  70503.4290554445,\n",
       "  70271.25330359243,\n",
       "  70038.81965104656,\n",
       "  69818.33108316675,\n",
       "  69608.74936059501,\n",
       "  69403.06560883114,\n",
       "  69196.9103436618,\n",
       "  69005.88333431326,\n",
       "  68802.0672514405,\n",
       "  68616.97607008467,\n",
       "  68440.14677651694,\n",
       "  68272.09831770344,\n",
       "  68125.15508143227,\n",
       "  67977.32160968368,\n",
       "  67842.75023518344,\n",
       "  67702.70101569849,\n",
       "  67561.28757422978,\n",
       "  67419.5962414746,\n",
       "  67284.16235007056,\n",
       "  67155.17925020578,\n",
       "  67031.72781558678,\n",
       "  66913.90344984713,\n",
       "  66789.4841251176,\n",
       "  66672.60947421801,\n",
       "  66554.8411262347,\n",
       "  66451.38299623707,\n",
       "  66359.30422815734,\n",
       "  66266.57174564911,\n",
       "  66179.45391506937,\n",
       "  66100.14604524341,\n",
       "  66014.47911277047,\n",
       "  65937.99740563265,\n",
       "  65862.51523364005,\n",
       "  65787.36831564558,\n",
       "  65718.6862101658,\n",
       "  65649.55539672507,\n",
       "  65581.72285101129,\n",
       "  65512.20976158279,\n",
       "  65448.149617091956,\n",
       "  65380.5296037159,\n",
       "  65316.36472542333,\n",
       "  65246.42648680033,\n",
       "  65174.35324920626,\n",
       "  65110.44598130292,\n",
       "  65042.51791803857,\n",
       "  64976.661196201785,\n",
       "  64910.15771989652,\n",
       "  64839.435449494355,\n",
       "  64777.9664275635,\n",
       "  64722.176946142994,\n",
       "  64664.12418420743,\n",
       "  64608.49140845484,\n",
       "  64554.04336562206,\n",
       "  64509.02856008937,\n",
       "  64461.22944717192,\n",
       "  64416.697458548915,\n",
       "  64368.970918097366,\n",
       "  64322.73663864064,\n",
       "  64280.48060839017,\n",
       "  64239.36865739652,\n",
       "  64198.35100026458,\n",
       "  64159.17785747883,\n",
       "  64116.36859860066,\n",
       "  64076.662607302445,\n",
       "  64034.7567982714,\n",
       "  63996.83201655104,\n",
       "  63958.88539216839,\n",
       "  63918.72296860301,\n",
       "  63882.80210599424,\n",
       "  63851.68833784101,\n",
       "  63819.08046948495,\n",
       "  63785.95376807973,\n",
       "  63750.77421654516,\n",
       "  63720.73009025164,\n",
       "  63689.18004762465,\n",
       "  63657.52125470367,\n",
       "  63625.59478627705,\n",
       "  63592.77743561853,\n",
       "  63562.50112447084,\n",
       "  63530.655156764464,\n",
       "  63500.362296419335,\n",
       "  63469.24968397225,\n",
       "  63438.57203227893,\n",
       "  63404.33787592604,\n",
       "  63371.91869818615,\n",
       "  63341.86308281397],\n",
       " 'mae': [136728.02,\n",
       "  136725.53,\n",
       "  136720.84,\n",
       "  136712.95,\n",
       "  136701.05,\n",
       "  136684.11,\n",
       "  136661.53,\n",
       "  136632.1,\n",
       "  136595.38,\n",
       "  136550.03,\n",
       "  136494.75,\n",
       "  136430.23,\n",
       "  136354.4,\n",
       "  136266.86,\n",
       "  136166.78,\n",
       "  136051.89,\n",
       "  135922.66,\n",
       "  135778.66,\n",
       "  135617.06,\n",
       "  135439.55,\n",
       "  135242.98,\n",
       "  135028.52,\n",
       "  134793.08,\n",
       "  134537.62,\n",
       "  134261.53,\n",
       "  133965.06,\n",
       "  133641.89,\n",
       "  133297.58,\n",
       "  132928.25,\n",
       "  132533.94,\n",
       "  132108.3,\n",
       "  131655.98,\n",
       "  131170.75,\n",
       "  130659.19,\n",
       "  130119.28,\n",
       "  129549.43,\n",
       "  128944.516,\n",
       "  128306.44,\n",
       "  127638.61,\n",
       "  126940.07,\n",
       "  126205.34,\n",
       "  125434.45,\n",
       "  124657.74,\n",
       "  123877.0,\n",
       "  123107.89,\n",
       "  122352.42,\n",
       "  121587.44,\n",
       "  120850.164,\n",
       "  120092.836,\n",
       "  119326.805,\n",
       "  118560.055,\n",
       "  117745.67,\n",
       "  116932.74,\n",
       "  116105.695,\n",
       "  115223.53,\n",
       "  114352.04,\n",
       "  113473.65,\n",
       "  112586.56,\n",
       "  111696.41,\n",
       "  110781.484,\n",
       "  109832.03,\n",
       "  108871.97,\n",
       "  107878.914,\n",
       "  106880.836,\n",
       "  105848.945,\n",
       "  104829.35,\n",
       "  103760.1,\n",
       "  102673.43,\n",
       "  101607.91,\n",
       "  100494.93,\n",
       "  99414.125,\n",
       "  98345.37,\n",
       "  97272.41,\n",
       "  96220.47,\n",
       "  95167.72,\n",
       "  94134.26,\n",
       "  93105.445,\n",
       "  92096.58,\n",
       "  91096.516,\n",
       "  90138.24,\n",
       "  89173.516,\n",
       "  88217.78,\n",
       "  87335.33,\n",
       "  86447.34,\n",
       "  85593.336,\n",
       "  84724.09,\n",
       "  83892.04,\n",
       "  83058.61,\n",
       "  82262.76,\n",
       "  81526.1,\n",
       "  80830.6,\n",
       "  80126.71,\n",
       "  79425.164,\n",
       "  78755.58,\n",
       "  78116.766,\n",
       "  77517.61,\n",
       "  76948.62,\n",
       "  76399.8,\n",
       "  75878.48,\n",
       "  75396.16,\n",
       "  74953.89,\n",
       "  74508.37,\n",
       "  74118.516,\n",
       "  73755.54,\n",
       "  73392.43,\n",
       "  73045.8,\n",
       "  72731.7,\n",
       "  72414.78,\n",
       "  72100.586,\n",
       "  71800.03,\n",
       "  71525.64,\n",
       "  71257.87,\n",
       "  71000.25,\n",
       "  70741.336,\n",
       "  70503.45,\n",
       "  70271.24,\n",
       "  70038.836,\n",
       "  69818.336,\n",
       "  69608.766,\n",
       "  69403.07,\n",
       "  69196.94,\n",
       "  69005.875,\n",
       "  68802.06,\n",
       "  68616.99,\n",
       "  68440.14,\n",
       "  68272.1,\n",
       "  68125.164,\n",
       "  67977.336,\n",
       "  67842.76,\n",
       "  67702.695,\n",
       "  67561.3,\n",
       "  67419.59,\n",
       "  67284.164,\n",
       "  67155.164,\n",
       "  67031.73,\n",
       "  66913.91,\n",
       "  66789.5,\n",
       "  66672.65,\n",
       "  66554.83,\n",
       "  66451.38,\n",
       "  66359.305,\n",
       "  66266.58,\n",
       "  66179.45,\n",
       "  66100.14,\n",
       "  66014.48,\n",
       "  65937.99,\n",
       "  65862.5,\n",
       "  65787.375,\n",
       "  65718.69,\n",
       "  65649.555,\n",
       "  65581.71,\n",
       "  65512.188,\n",
       "  65448.156,\n",
       "  65380.54,\n",
       "  65316.363,\n",
       "  65246.434,\n",
       "  65174.336,\n",
       "  65110.44,\n",
       "  65042.52,\n",
       "  64976.656,\n",
       "  64910.156,\n",
       "  64839.43,\n",
       "  64777.973,\n",
       "  64722.184,\n",
       "  64664.13,\n",
       "  64608.504,\n",
       "  64554.03,\n",
       "  64509.035,\n",
       "  64461.23,\n",
       "  64416.69,\n",
       "  64368.973,\n",
       "  64322.74,\n",
       "  64280.49,\n",
       "  64239.367,\n",
       "  64198.375,\n",
       "  64159.188,\n",
       "  64116.387,\n",
       "  64076.66,\n",
       "  64034.77,\n",
       "  63996.848,\n",
       "  63958.895,\n",
       "  63918.727,\n",
       "  63882.8,\n",
       "  63851.688,\n",
       "  63819.07,\n",
       "  63785.957,\n",
       "  63750.79,\n",
       "  63720.73,\n",
       "  63689.16,\n",
       "  63657.535,\n",
       "  63625.59,\n",
       "  63592.76,\n",
       "  63562.492,\n",
       "  63530.664,\n",
       "  63500.363,\n",
       "  63469.23,\n",
       "  63438.57,\n",
       "  63404.36,\n",
       "  63371.914,\n",
       "  63341.863]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看看字典裡有哪些欄位\n",
    "mae_history = history.history\n",
    "mae_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#建構平均k折驗證分數的歷史\n",
    "average_mae_history=[\n",
    "    np.mean([x[i] for x in all_mae_histories])for i in range(num_epochs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#mae 繪圖\n",
    "plt.plot(range(1, len(average_mae_history)+1),average_mae_history)\n",
    "plt.xlabel('Epohs')\n",
    "plt.xlabel('Loss')\n",
    "plt.show()\n",
    "#接著選取MAE最小的周期再作訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[135647.28125,\n",
       " 135643.5234375,\n",
       " 135637.08203125,\n",
       " 135627.01171875,\n",
       " 135612.328125,\n",
       " 135592.0625,\n",
       " 135565.296875,\n",
       " 135531.21875,\n",
       " 135489.00390625,\n",
       " 135437.4375,\n",
       " 135375.8359375,\n",
       " 135303.47265625,\n",
       " 135219.0859375,\n",
       " 135121.8671875,\n",
       " 135010.98046875,\n",
       " 134884.8828125,\n",
       " 134743.515625,\n",
       " 134585.47265625,\n",
       " 134410.22265625,\n",
       " 134216.5859375,\n",
       " 134003.330078125,\n",
       " 133771.36328125,\n",
       " 133516.880859375,\n",
       " 133240.91015625,\n",
       " 132941.40234375,\n",
       " 132618.8359375,\n",
       " 132270.7578125,\n",
       " 131897.6171875,\n",
       " 131498.544921875,\n",
       " 131070.185546875,\n",
       " 130613.0078125,\n",
       " 130124.37109375,\n",
       " 129608.130859375,\n",
       " 129059.54296875,\n",
       " 128476.583984375,\n",
       " 127862.3125,\n",
       " 127215.3359375,\n",
       " 126542.59375,\n",
       " 125857.26171875,\n",
       " 125174.130859375,\n",
       " 124496.140625,\n",
       " 123820.9375,\n",
       " 123143.890625,\n",
       " 122461.98046875,\n",
       " 121776.642578125,\n",
       " 121086.201171875,\n",
       " 120384.359375,\n",
       " 119669.490234375,\n",
       " 118943.759765625,\n",
       " 118198.345703125,\n",
       " 117432.9921875,\n",
       " 116643.404296875,\n",
       " 115844.21484375,\n",
       " 115029.83984375,\n",
       " 114200.2578125,\n",
       " 113365.54296875,\n",
       " 112514.388671875,\n",
       " 111625.68359375,\n",
       " 110732.916015625,\n",
       " 109810.6171875,\n",
       " 108855.857421875,\n",
       " 107891.38671875,\n",
       " 106893.30078125,\n",
       " 105872.2734375,\n",
       " 104836.15625,\n",
       " 103792.69921875,\n",
       " 102749.6171875,\n",
       " 101685.0234375,\n",
       " 100625.4140625,\n",
       " 99558.70703125,\n",
       " 98511.787109375,\n",
       " 97456.232421875,\n",
       " 96427.072265625,\n",
       " 95393.017578125,\n",
       " 94402.224609375,\n",
       " 93418.080078125,\n",
       " 92438.9921875,\n",
       " 91473.91015625,\n",
       " 90525.416015625,\n",
       " 89611.5234375,\n",
       " 88711.560546875,\n",
       " 87816.396484375,\n",
       " 86944.400390625,\n",
       " 86081.435546875,\n",
       " 85235.833984375,\n",
       " 84387.79296875,\n",
       " 83570.509765625,\n",
       " 82760.125,\n",
       " 81974.10546875,\n",
       " 81192.4140625,\n",
       " 80425.759765625,\n",
       " 79675.0546875,\n",
       " 78935.345703125,\n",
       " 78204.39453125,\n",
       " 77482.30078125,\n",
       " 76779.91015625,\n",
       " 76113.55859375,\n",
       " 75471.462890625,\n",
       " 74851.888671875,\n",
       " 74256.875,\n",
       " 73687.80859375,\n",
       " 73144.087890625,\n",
       " 72645.734375,\n",
       " 72153.15625,\n",
       " 71677.099609375,\n",
       " 71221.51953125,\n",
       " 70780.583984375,\n",
       " 70351.9150390625,\n",
       " 69949.3876953125,\n",
       " 69552.09375,\n",
       " 69181.8125,\n",
       " 68819.990234375,\n",
       " 68477.509765625,\n",
       " 68151.5732421875,\n",
       " 67851.9306640625,\n",
       " 67551.6591796875,\n",
       " 67268.0498046875,\n",
       " 66990.61328125,\n",
       " 66734.0107421875,\n",
       " 66490.4951171875,\n",
       " 66262.83203125,\n",
       " 66031.78125,\n",
       " 65806.4267578125,\n",
       " 65588.982421875,\n",
       " 65382.716796875,\n",
       " 65193.248046875,\n",
       " 65018.90234375,\n",
       " 64853.27734375,\n",
       " 64695.0,\n",
       " 64546.1103515625,\n",
       " 64404.26171875,\n",
       " 64254.3388671875,\n",
       " 64112.6865234375,\n",
       " 63983.994140625,\n",
       " 63860.1376953125,\n",
       " 63743.71875,\n",
       " 63636.3076171875,\n",
       " 63533.404296875,\n",
       " 63433.390625,\n",
       " 63337.21484375,\n",
       " 63242.9892578125,\n",
       " 63151.3369140625,\n",
       " 63067.96875,\n",
       " 62984.15234375,\n",
       " 62901.677734375,\n",
       " 62820.607421875,\n",
       " 62744.140625,\n",
       " 62667.5185546875,\n",
       " 62597.900390625,\n",
       " 62527.7158203125,\n",
       " 62455.7568359375,\n",
       " 62385.876953125,\n",
       " 62324.759765625,\n",
       " 62260.84765625,\n",
       " 62199.87109375,\n",
       " 62137.7822265625,\n",
       " 62072.8193359375,\n",
       " 62015.802734375,\n",
       " 61957.7744140625,\n",
       " 61908.3505859375,\n",
       " 61865.4814453125,\n",
       " 61821.1357421875,\n",
       " 61782.46484375,\n",
       " 61740.6572265625,\n",
       " 61699.3037109375,\n",
       " 61662.9970703125,\n",
       " 61634.15625,\n",
       " 61600.43359375,\n",
       " 61568.6796875,\n",
       " 61538.228515625,\n",
       " 61509.599609375,\n",
       " 61482.9677734375,\n",
       " 61459.408203125,\n",
       " 61438.8125,\n",
       " 61416.4052734375,\n",
       " 61394.2265625,\n",
       " 61373.76171875,\n",
       " 61354.7998046875,\n",
       " 61335.2861328125,\n",
       " 61316.6513671875,\n",
       " 61298.7978515625,\n",
       " 61281.201171875,\n",
       " 61262.607421875,\n",
       " 61243.5546875,\n",
       " 61229.1484375,\n",
       " 61211.0009765625,\n",
       " 61193.5869140625,\n",
       " 61178.6806640625,\n",
       " 61164.5537109375,\n",
       " 61148.9228515625,\n",
       " 61131.876953125,\n",
       " 61117.2314453125,\n",
       " 61102.7666015625,\n",
       " 61089.1025390625,\n",
       " 61075.927734375,\n",
       " 61061.544921875,\n",
       " 61046.2958984375,\n",
       " 61033.390625,\n",
       " 61019.5302734375,\n",
       " 61005.7529296875]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_mae_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1417/1417 [==============================] - 0s 139us/step - loss: 135666.6109 - mae: 135666.6562\n",
      "Epoch 2/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 135664.6347 - mae: 135664.5938\n",
      "Epoch 3/60\n",
      "1417/1417 [==============================] - 0s 78us/step - loss: 135661.1252 - mae: 135661.1250\n",
      "Epoch 4/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 135654.8468 - mae: 135654.8594\n",
      "Epoch 5/60\n",
      "1417/1417 [==============================] - 0s 82us/step - loss: 135644.9069 - mae: 135644.9062\n",
      "Epoch 6/60\n",
      "1417/1417 [==============================] - 0s 81us/step - loss: 135630.1918 - mae: 135630.1719\n",
      "Epoch 7/60\n",
      "1417/1417 [==============================] - 0s 79us/step - loss: 135609.7972 - mae: 135609.7969\n",
      "Epoch 8/60\n",
      "1417/1417 [==============================] - 0s 81us/step - loss: 135582.3983 - mae: 135582.4062\n",
      "Epoch 9/60\n",
      "1417/1417 [==============================] - 0s 81us/step - loss: 135546.9842 - mae: 135546.9844\n",
      "Epoch 10/60\n",
      "1417/1417 [==============================] - 0s 83us/step - loss: 135502.8964 - mae: 135502.9219\n",
      "Epoch 11/60\n",
      "1417/1417 [==============================] - 0s 82us/step - loss: 135448.4468 - mae: 135448.4531\n",
      "Epoch 12/60\n",
      "1417/1417 [==============================] - 0s 81us/step - loss: 135382.9037 - mae: 135382.9375\n",
      "Epoch 13/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 135305.9427 - mae: 135305.9062\n",
      "Epoch 14/60\n",
      "1417/1417 [==============================] - 0s 82us/step - loss: 135215.6625 - mae: 135215.6562\n",
      "Epoch 15/60\n",
      "1417/1417 [==============================] - 0s 81us/step - loss: 135110.9737 - mae: 135110.9531\n",
      "Epoch 16/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 134991.9456 - mae: 134991.9062\n",
      "Epoch 17/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 134856.2610 - mae: 134856.2500\n",
      "Epoch 18/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 134703.0609 - mae: 134703.0781\n",
      "Epoch 19/60\n",
      "1417/1417 [==============================] - 0s 79us/step - loss: 134531.0515 - mae: 134531.0312\n",
      "Epoch 20/60\n",
      "1417/1417 [==============================] - 0s 79us/step - loss: 134339.3656 - mae: 134339.3750\n",
      "Epoch 21/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 134126.6320 - mae: 134126.6406\n",
      "Epoch 22/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 133893.5595 - mae: 133893.5938\n",
      "Epoch 23/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 133637.7021 - mae: 133637.6875\n",
      "Epoch 24/60\n",
      "1417/1417 [==============================] - 0s 82us/step - loss: 133357.5125 - mae: 133357.5000\n",
      "Epoch 25/60\n",
      "1417/1417 [==============================] - 0s 85us/step - loss: 133053.5576 - mae: 133053.5781\n",
      "Epoch 26/60\n",
      "1417/1417 [==============================] - 0s 82us/step - loss: 132726.3328 - mae: 132726.3438\n",
      "Epoch 27/60\n",
      "1417/1417 [==============================] - 0s 78us/step - loss: 132371.8189 - mae: 132371.7969\n",
      "Epoch 28/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 131988.8306 - mae: 131988.7969\n",
      "Epoch 29/60\n",
      "1417/1417 [==============================] - 0s 84us/step - loss: 131575.6521 - mae: 131575.6094\n",
      "Epoch 30/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 131134.7957 - mae: 131134.8125\n",
      "Epoch 31/60\n",
      "1417/1417 [==============================] - 0s 78us/step - loss: 130660.6545 - mae: 130660.6406\n",
      "Epoch 32/60\n",
      "1417/1417 [==============================] - 0s 81us/step - loss: 130157.7985 - mae: 130157.8203\n",
      "Epoch 33/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 129619.5825 - mae: 129619.5781\n",
      "Epoch 34/60\n",
      "1417/1417 [==============================] - 0s 83us/step - loss: 129041.8170 - mae: 129041.8594\n",
      "Epoch 35/60\n",
      "1417/1417 [==============================] - 0s 85us/step - loss: 128436.5721 - mae: 128436.5469\n",
      "Epoch 36/60\n",
      "1417/1417 [==============================] - 0s 85us/step - loss: 127790.8965 - mae: 127790.9062\n",
      "Epoch 37/60\n",
      "1417/1417 [==============================] - 0s 79us/step - loss: 127110.6969 - mae: 127110.6875\n",
      "Epoch 38/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 126390.8040 - mae: 126390.8203\n",
      "Epoch 39/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 125630.7426 - mae: 125630.7500\n",
      "Epoch 40/60\n",
      "1417/1417 [==============================] - 0s 82us/step - loss: 124826.3790 - mae: 124826.3750\n",
      "Epoch 41/60\n",
      "1417/1417 [==============================] - 0s 85us/step - loss: 123988.3680 - mae: 123988.3438\n",
      "Epoch 42/60\n",
      "1417/1417 [==============================] - 0s 83us/step - loss: 123115.4850 - mae: 123115.4844\n",
      "Epoch 43/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 122245.1505 - mae: 122245.1484\n",
      "Epoch 44/60\n",
      "1417/1417 [==============================] - 0s 81us/step - loss: 121401.2836 - mae: 121401.3047\n",
      "Epoch 45/60\n",
      "1417/1417 [==============================] - 0s 81us/step - loss: 120560.7984 - mae: 120560.7812\n",
      "Epoch 46/60\n",
      "1417/1417 [==============================] - 0s 81us/step - loss: 119745.1675 - mae: 119745.1719\n",
      "Epoch 47/60\n",
      "1417/1417 [==============================] - 0s 85us/step - loss: 118924.0452 - mae: 118924.0781\n",
      "Epoch 48/60\n",
      "1417/1417 [==============================] - 0s 82us/step - loss: 118106.0569 - mae: 118106.0469\n",
      "Epoch 49/60\n",
      "1417/1417 [==============================] - 0s 88us/step - loss: 117250.0032 - mae: 117249.9844\n",
      "Epoch 50/60\n",
      "1417/1417 [==============================] - 0s 81us/step - loss: 116371.7750 - mae: 116371.7812\n",
      "Epoch 51/60\n",
      "1417/1417 [==============================] - 0s 85us/step - loss: 115469.7134 - mae: 115469.7109\n",
      "Epoch 52/60\n",
      "1417/1417 [==============================] - 0s 91us/step - loss: 114567.2286 - mae: 114567.1953\n",
      "Epoch 53/60\n",
      "1417/1417 [==============================] - 0s 83us/step - loss: 113639.1129 - mae: 113639.1172\n",
      "Epoch 54/60\n",
      "1417/1417 [==============================] - 0s 80us/step - loss: 112698.8310 - mae: 112698.8594\n",
      "Epoch 55/60\n",
      "1417/1417 [==============================] - 0s 85us/step - loss: 111753.4852 - mae: 111753.5312\n",
      "Epoch 56/60\n",
      "1417/1417 [==============================] - 0s 82us/step - loss: 110789.1883 - mae: 110789.1953\n",
      "Epoch 57/60\n",
      "1417/1417 [==============================] - 0s 87us/step - loss: 109797.0972 - mae: 109797.1016\n",
      "Epoch 58/60\n",
      "1417/1417 [==============================] - 0s 89us/step - loss: 108804.5204 - mae: 108804.4609\n",
      "Epoch 59/60\n",
      "1417/1417 [==============================] - 0s 86us/step - loss: 107769.1721 - mae: 107769.2031\n",
      "Epoch 60/60\n",
      "1417/1417 [==============================] - 0s 83us/step - loss: 106718.9543 - mae: 106718.9688\n",
      "608/608 [==============================] - 0s 35us/step\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(train_data, train_targets,epochs=60, batch_size=8,)\n",
    "\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103239.484375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mae_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369     200000\n",
       "991     100000\n",
       "1497     50000\n",
       "292     250000\n",
       "1586     50000\n",
       "387     200000\n",
       "1905     20000\n",
       "301     248000\n",
       "1796     20000\n",
       "219     300000\n",
       "655     150000\n",
       "116     360000\n",
       "244     280000\n",
       "1477     60000\n",
       "1187     80000\n",
       "1990     20000\n",
       "1133     95000\n",
       "481     190000\n",
       "263     260000\n",
       "902     120000\n",
       "840     120000\n",
       "1671     50000\n",
       "782     132000\n",
       "508     180000\n",
       "869     120000\n",
       "44      500000\n",
       "412     200000\n",
       "833     125000\n",
       "1401     60000\n",
       "10      500000\n",
       "         ...  \n",
       "1206     80000\n",
       "433     200000\n",
       "1895     20000\n",
       "439     200000\n",
       "35      500000\n",
       "1257     80000\n",
       "817     130000\n",
       "458     200000\n",
       "448     200000\n",
       "445     200000\n",
       "4       500000\n",
       "363     200000\n",
       "341     210000\n",
       "503     180000\n",
       "726     150000\n",
       "1341     70000\n",
       "251     280000\n",
       "1625     50000\n",
       "385     200000\n",
       "1096    100000\n",
       "1085    100000\n",
       "1585     50000\n",
       "221     300000\n",
       "1763     30000\n",
       "891     120000\n",
       "132     350000\n",
       "383     200000\n",
       "1577     50000\n",
       "1567     50000\n",
       "326     220000\n",
       "Name: credLimit, Length: 2025, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"credLimit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看起來成效不大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試 小綠同學"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>serveTime</th>\n",
       "      <th>Loan</th>\n",
       "      <th>SalPerY</th>\n",
       "      <th>holdCard</th>\n",
       "      <th>Career</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>120</td>\n",
       "      <td>4</td>\n",
       "      <td>600000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age serveTime Loan SalPerY holdCard Career\n",
       "0   8       120    4  600000        1      1\n",
       "1  28        12    0  600000        0      0\n",
       "2  28        12    0      87        2      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "小綠 = pd.DataFrame(columns=[\"age\",\"serveTime\",\"Loan\",\"SalPerY\",\"holdCard\",\"Career\"])\n",
    "小綠.loc[0]=8,120,4,600000,1,1\n",
    "小綠.loc[1]=280,120,3,600000000,1,1\n",
    "小綠.loc[2]=28,12,0,87,2,0\n",
    "小綠.loc[3]=28,12,0,8700,2,0\n",
    "小綠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
