{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense,SimpleRNN\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import word2vec\n",
    "import gensim.models\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定義斷詞與整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斷詞套件\n",
    "def 斷詞(news):\n",
    "    df1=news\n",
    "    df1['seg']=None\n",
    "    df1['word_freq']=None\n",
    "    n=len(news)\n",
    "    alltext=[]\n",
    "    stopset=set()\n",
    "    stop2=['/n','']\n",
    "    with open('C:\\\\Users\\\\Big data\\\\Desktop\\\\字詞貼標\\\\DB104-master\\\\stop.txt','r',encoding='ISO-8859-1') as s:\n",
    "        for line in s:\n",
    "            stopset.add(line.strip('\\n'))\n",
    "    jieba.load_userdict('C:\\\\Users\\\\Big data\\\\Desktop\\\\字詞貼標\\\\DB104-master\\\\userdict.txt')\n",
    "    for i in range(n):\n",
    "        seg=''\n",
    "        wf={}\n",
    "        text= str(df1.loc[i]['content'])\n",
    "        cut=jieba.cut(text,cut_all=False)\n",
    "        for j in cut:\n",
    "            if j not in stopset:\n",
    "                seg +=j+' '\n",
    "                seg = seg.replace(\"\\n\",'')\n",
    "        df1['seg'][i]=seg\n",
    "        for w in seg.split(' '):\n",
    "            if w not in wf:\n",
    "                wf[w]=1\n",
    "            else:\n",
    "                wf[w]+=1\n",
    "        word_list=[(k,wf[k]) for k in wf if k not in stop2 ]\n",
    "        word_list.sort(key=lambda a :a[1],reverse=True)\n",
    "        df1['word_freq'][i]=word_list\n",
    "    return(df1)\n",
    "\n",
    "\n",
    "\n",
    "# 斷詞整理(去除標點符號)\n",
    "def 整理(df1):\n",
    "    datanews = df1[\"seg\"]\n",
    "    # 轉list\n",
    "    train_data = np.array(datanews)#np.ndarray()\n",
    "    datanews=train_data.tolist()#list\n",
    "    df1=''\n",
    "    for datanew in datanews:\n",
    "        df1+=str(datanew)+'\\n'\n",
    "    df1=df1.replace('[','').replace('\\'','').replace('「 ','').replace('」 ','').replace('」 ','').replace('、 ','').replace('： ','')\n",
    "    df1=df1.replace('《 ','').replace('》 ','').replace('） ','').replace('( ','').replace('／ ','').replace('， ','')\n",
    "    df1=df1.replace('╱ ','').replace('！ ','').replace('？ ','').replace('（ ','').replace('。 ','').replace('； ','').replace('… ','')\n",
    "    df1=df1.replace(']','').replace(' :','').replace('\\u3000','').replace('\\n','')\n",
    "    df2=df1\n",
    "    for i in range(len(df2)):\n",
    "        df1=' '.join(df2.split())\n",
    "    return(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定義word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練語意空間\n",
    "#去除只出現一次的詞\n",
    "size = [50,100,150]\n",
    "cbow = [0,1] #(context as input, target word as output)\n",
    "for x in size:\n",
    "    for z in cbow:\n",
    "        y = 15\n",
    "        sentences = word2vec.LineSentence(r\"C:\\Users\\Big data\\Desktop\\class\\funcardproject\\data\\文章\\一整大篇_乾淨.txt\")\n",
    "        model = word2vec.Word2Vec(sentences,size=x,window=y,min_count=1,sg = z)\n",
    "        BIN =  r'C:\\Users\\Big data\\Desktop\\class\\funcardproject\\word2vec_model\\article_social\\article_social維度'+str(x)+'win'+str(y)+'cbow'+str(z)+'.bin'\n",
    "        model.wv.save_word2vec_format(BIN, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 執行斷詞與整理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\BIGDAT~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.817 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#開啟要整理的檔案\n",
    "news = pd.read_excel(r\"C:\\Users\\Big data\\Desktop\\class\\funcardproject\\data\\豪哥\\DB104-hau\\integrate\\card_article\\excel_file\\article_social.xlsx\")\n",
    "#斷詞\n",
    "df1=斷詞(news)\n",
    "df1\n",
    "news = 整理(df1)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存excel檔\n",
    "news.to_excel(r\"C:\\Users\\Big data\\Desktop\\class\\funcardproject\\data\\文章\\討論\\article_social_斷詞完.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存成TXT\n",
    "with open(r\"C:\\Users\\Big data\\Desktop\\class\\funcardproject\\data\\文章\\討論\\article_social_一整大篇.txt\",\"w\",encoding=\"utf-8\")as f:\n",
    "    f.write(str(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 執行建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
